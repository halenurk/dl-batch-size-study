{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tf-cpu/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "55000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "print(mnist.train.num_examples) # Number of training data\n",
    "print(mnist.test.num_examples) # Number of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "learningrate = 0.02#0.005\n",
    "num_of_epochs=50\n",
    "batchsize = 128\n",
    "num_of_iters=55000/batchsize*num_of_epochs\n",
    "\n",
    "\n",
    "noutput = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, noutput])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # reshape input to 28x28 size\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution layer 1\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    # Max pooling\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution layer 2\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    # Max pooling\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "\n",
    "init_var=tf.contrib.layers.variance_scaling_initializer()\n",
    "init_xavier=tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "weights = {\n",
    "    'wc1': tf.get_variable(\"wc1\", shape=[5, 5, 1, 32],initializer=init_var),\n",
    "    'wc2': tf.get_variable(\"wc2\", shape=[5, 5, 32, 64],initializer=init_var),\n",
    "    'wd1': tf.get_variable(\"wd1\", shape=[7*7*64, 1024],initializer=init_var),\n",
    "    'out': tf.get_variable(\"out\", shape=[1024,noutput],initializer=init_var)\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.zeros([32])),\n",
    "    'bc2': tf.Variable(tf.zeros([64])),\n",
    "    'bd1': tf.Variable(tf.zeros([1024])),\n",
    "    'out': tf.Variable(tf.zeros([noutput]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = conv_net(X, weights, biases, dropout=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = model, labels=Y))\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=learningrate)\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "optimizer=tf.train.MomentumOptimizer(learning_rate=learningrate, momentum=0.99)\n",
    "\n",
    "train_min = optimizer.minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "tf.summary.scalar(\"trainingLoss\", loss)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "filename = \"./log/mnist_project_batchsize128\" #_lr0p005\n",
    "writer = tf.summary.FileWriter(filename, tf.get_default_graph())\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1, Training Loss: 2.2580, Training Accuracy: 0.2109\n",
      "Iteration: 11, Training Loss: 0.7471, Training Accuracy: 0.7891\n",
      "Iteration: 21, Training Loss: 0.6078, Training Accuracy: 0.7891\n",
      "Iteration: 31, Training Loss: 0.5925, Training Accuracy: 0.7812\n",
      "Iteration: 41, Training Loss: 0.4588, Training Accuracy: 0.8828\n",
      "Iteration: 51, Training Loss: 0.3728, Training Accuracy: 0.9062\n",
      "Iteration: 61, Training Loss: 0.2104, Training Accuracy: 0.9375\n",
      "Iteration: 71, Training Loss: 0.3469, Training Accuracy: 0.8984\n",
      "Iteration: 81, Training Loss: 0.2322, Training Accuracy: 0.9453\n",
      "Iteration: 91, Training Loss: 0.0722, Training Accuracy: 0.9766\n",
      "Iteration: 101, Training Loss: 0.2482, Training Accuracy: 0.9453\n",
      "Iteration: 111, Training Loss: 0.1617, Training Accuracy: 0.9766\n",
      "Iteration: 121, Training Loss: 0.2618, Training Accuracy: 0.9219\n",
      "Iteration: 131, Training Loss: 0.3769, Training Accuracy: 0.8750\n",
      "Iteration: 141, Training Loss: 0.1596, Training Accuracy: 0.9453\n",
      "Iteration: 151, Training Loss: 0.1541, Training Accuracy: 0.9531\n",
      "Iteration: 161, Training Loss: 0.1495, Training Accuracy: 0.9531\n",
      "Iteration: 171, Training Loss: 0.4882, Training Accuracy: 0.9141\n",
      "Iteration: 181, Training Loss: 0.2064, Training Accuracy: 0.9453\n",
      "Iteration: 191, Training Loss: 0.1770, Training Accuracy: 0.9609\n",
      "Iteration: 201, Training Loss: 0.1450, Training Accuracy: 0.9766\n",
      "Iteration: 211, Training Loss: 0.1070, Training Accuracy: 0.9688\n",
      "Iteration: 221, Training Loss: 0.2119, Training Accuracy: 0.9219\n",
      "Iteration: 231, Training Loss: 0.1809, Training Accuracy: 0.9297\n",
      "Iteration: 241, Training Loss: 0.1149, Training Accuracy: 0.9766\n",
      "Iteration: 251, Training Loss: 0.1785, Training Accuracy: 0.9453\n",
      "Iteration: 261, Training Loss: 0.1697, Training Accuracy: 0.9297\n",
      "Iteration: 271, Training Loss: 0.1600, Training Accuracy: 0.9531\n",
      "Iteration: 281, Training Loss: 0.0786, Training Accuracy: 0.9766\n",
      "Iteration: 291, Training Loss: 0.1503, Training Accuracy: 0.9609\n",
      "Iteration: 301, Training Loss: 0.0887, Training Accuracy: 0.9609\n",
      "Iteration: 311, Training Loss: 0.0721, Training Accuracy: 0.9844\n",
      "Iteration: 321, Training Loss: 0.1058, Training Accuracy: 0.9609\n",
      "Iteration: 331, Training Loss: 0.1482, Training Accuracy: 0.9453\n",
      "Iteration: 341, Training Loss: 0.0409, Training Accuracy: 0.9766\n",
      "Iteration: 351, Training Loss: 0.0286, Training Accuracy: 0.9922\n",
      "Iteration: 361, Training Loss: 0.0763, Training Accuracy: 0.9531\n",
      "Iteration: 371, Training Loss: 0.2131, Training Accuracy: 0.9531\n",
      "Iteration: 381, Training Loss: 0.0925, Training Accuracy: 0.9688\n",
      "Iteration: 391, Training Loss: 0.1647, Training Accuracy: 0.9688\n",
      "Iteration: 401, Training Loss: 0.1510, Training Accuracy: 0.9688\n",
      "Iteration: 411, Training Loss: 0.2547, Training Accuracy: 0.9609\n",
      "Iteration: 421, Training Loss: 0.0951, Training Accuracy: 0.9844\n",
      "Iteration: 431, Training Loss: 0.1270, Training Accuracy: 0.9531\n",
      "Iteration: 441, Training Loss: 0.1364, Training Accuracy: 0.9688\n",
      "Iteration: 451, Training Loss: 0.1458, Training Accuracy: 0.9609\n",
      "Iteration: 461, Training Loss: 0.1749, Training Accuracy: 0.9531\n",
      "Iteration: 471, Training Loss: 0.0647, Training Accuracy: 0.9688\n",
      "Iteration: 481, Training Loss: 0.2992, Training Accuracy: 0.9609\n",
      "Iteration: 491, Training Loss: 0.0522, Training Accuracy: 0.9844\n",
      "Iteration: 501, Training Loss: 0.0938, Training Accuracy: 0.9844\n",
      "Iteration: 511, Training Loss: 0.0529, Training Accuracy: 0.9766\n",
      "Iteration: 521, Training Loss: 0.0972, Training Accuracy: 0.9766\n",
      "Iteration: 531, Training Loss: 0.1623, Training Accuracy: 0.9609\n",
      "Iteration: 541, Training Loss: 0.0689, Training Accuracy: 0.9922\n",
      "Iteration: 551, Training Loss: 0.0416, Training Accuracy: 0.9766\n",
      "Iteration: 561, Training Loss: 0.1242, Training Accuracy: 0.9688\n",
      "Iteration: 571, Training Loss: 0.1151, Training Accuracy: 0.9609\n",
      "Iteration: 581, Training Loss: 0.1701, Training Accuracy: 0.9531\n",
      "Iteration: 591, Training Loss: 0.0512, Training Accuracy: 0.9766\n",
      "Iteration: 601, Training Loss: 0.2551, Training Accuracy: 0.9141\n",
      "Iteration: 611, Training Loss: 0.1008, Training Accuracy: 0.9609\n",
      "Iteration: 621, Training Loss: 0.0463, Training Accuracy: 0.9844\n",
      "Iteration: 631, Training Loss: 0.1022, Training Accuracy: 0.9531\n",
      "Iteration: 641, Training Loss: 0.1370, Training Accuracy: 0.9609\n",
      "Iteration: 651, Training Loss: 0.0900, Training Accuracy: 0.9609\n",
      "Iteration: 661, Training Loss: 0.1853, Training Accuracy: 0.9531\n",
      "Iteration: 671, Training Loss: 0.0636, Training Accuracy: 0.9766\n",
      "Iteration: 681, Training Loss: 0.1974, Training Accuracy: 0.9531\n",
      "Iteration: 691, Training Loss: 0.1941, Training Accuracy: 0.9531\n",
      "Iteration: 701, Training Loss: 0.1705, Training Accuracy: 0.9609\n",
      "Iteration: 711, Training Loss: 0.0407, Training Accuracy: 0.9922\n",
      "Iteration: 721, Training Loss: 0.2202, Training Accuracy: 0.9531\n",
      "Iteration: 731, Training Loss: 0.0694, Training Accuracy: 0.9844\n",
      "Iteration: 741, Training Loss: 0.1446, Training Accuracy: 0.9375\n",
      "Iteration: 751, Training Loss: 0.0256, Training Accuracy: 1.0000\n",
      "Iteration: 761, Training Loss: 0.2048, Training Accuracy: 0.9531\n",
      "Iteration: 771, Training Loss: 0.1265, Training Accuracy: 0.9531\n",
      "Iteration: 781, Training Loss: 0.0539, Training Accuracy: 0.9766\n",
      "Iteration: 791, Training Loss: 0.3207, Training Accuracy: 0.9297\n",
      "Iteration: 801, Training Loss: 0.0455, Training Accuracy: 0.9844\n",
      "Iteration: 811, Training Loss: 0.1194, Training Accuracy: 0.9688\n",
      "Iteration: 821, Training Loss: 0.1158, Training Accuracy: 0.9766\n",
      "Iteration: 831, Training Loss: 0.2103, Training Accuracy: 0.9609\n",
      "Iteration: 841, Training Loss: 0.0669, Training Accuracy: 0.9688\n",
      "Iteration: 851, Training Loss: 0.1128, Training Accuracy: 0.9766\n",
      "Iteration: 861, Training Loss: 0.0674, Training Accuracy: 0.9766\n",
      "Iteration: 871, Training Loss: 0.0998, Training Accuracy: 0.9609\n",
      "Iteration: 881, Training Loss: 0.0782, Training Accuracy: 0.9766\n",
      "Iteration: 891, Training Loss: 0.1526, Training Accuracy: 0.9453\n",
      "Iteration: 901, Training Loss: 0.0861, Training Accuracy: 0.9844\n",
      "Iteration: 911, Training Loss: 0.0712, Training Accuracy: 0.9766\n",
      "Iteration: 921, Training Loss: 0.1097, Training Accuracy: 0.9766\n",
      "Iteration: 931, Training Loss: 0.0419, Training Accuracy: 0.9844\n",
      "Iteration: 941, Training Loss: 0.1356, Training Accuracy: 0.9609\n",
      "Iteration: 951, Training Loss: 0.1254, Training Accuracy: 0.9688\n",
      "Iteration: 961, Training Loss: 0.1859, Training Accuracy: 0.9609\n",
      "Iteration: 971, Training Loss: 0.0300, Training Accuracy: 0.9922\n",
      "Iteration: 981, Training Loss: 0.0213, Training Accuracy: 0.9922\n",
      "Iteration: 991, Training Loss: 0.1982, Training Accuracy: 0.9688\n",
      "Iteration: 1001, Training Loss: 0.1053, Training Accuracy: 0.9688\n",
      "Iteration: 1011, Training Loss: 0.1160, Training Accuracy: 0.9609\n",
      "Iteration: 1021, Training Loss: 0.0552, Training Accuracy: 0.9844\n",
      "Iteration: 1031, Training Loss: 0.0654, Training Accuracy: 0.9844\n",
      "Iteration: 1041, Training Loss: 0.1303, Training Accuracy: 0.9609\n",
      "Iteration: 1051, Training Loss: 0.1747, Training Accuracy: 0.9766\n",
      "Iteration: 1061, Training Loss: 0.0539, Training Accuracy: 0.9766\n",
      "Iteration: 1071, Training Loss: 0.0858, Training Accuracy: 0.9688\n",
      "Iteration: 1081, Training Loss: 0.0823, Training Accuracy: 0.9766\n",
      "Iteration: 1091, Training Loss: 0.0659, Training Accuracy: 0.9609\n",
      "Iteration: 1101, Training Loss: 0.1399, Training Accuracy: 0.9609\n",
      "Iteration: 1111, Training Loss: 0.0781, Training Accuracy: 0.9766\n",
      "Iteration: 1121, Training Loss: 0.0853, Training Accuracy: 0.9688\n",
      "Iteration: 1131, Training Loss: 0.0355, Training Accuracy: 0.9766\n",
      "Iteration: 1141, Training Loss: 0.2333, Training Accuracy: 0.9375\n",
      "Iteration: 1151, Training Loss: 0.2550, Training Accuracy: 0.9453\n",
      "Iteration: 1161, Training Loss: 0.0702, Training Accuracy: 0.9766\n",
      "Iteration: 1171, Training Loss: 0.1174, Training Accuracy: 0.9453\n",
      "Iteration: 1181, Training Loss: 0.1021, Training Accuracy: 0.9922\n",
      "Iteration: 1191, Training Loss: 0.2282, Training Accuracy: 0.9609\n",
      "Iteration: 1201, Training Loss: 0.0192, Training Accuracy: 0.9922\n",
      "Iteration: 1211, Training Loss: 0.1117, Training Accuracy: 0.9688\n",
      "Iteration: 1221, Training Loss: 0.1349, Training Accuracy: 0.9609\n",
      "Iteration: 1231, Training Loss: 0.0315, Training Accuracy: 0.9844\n",
      "Iteration: 1241, Training Loss: 0.0191, Training Accuracy: 0.9844\n",
      "Iteration: 1251, Training Loss: 0.3211, Training Accuracy: 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1261, Training Loss: 0.0750, Training Accuracy: 0.9844\n",
      "Iteration: 1271, Training Loss: 0.0911, Training Accuracy: 0.9766\n",
      "Iteration: 1281, Training Loss: 0.0768, Training Accuracy: 0.9688\n",
      "Iteration: 1291, Training Loss: 0.1750, Training Accuracy: 0.9609\n",
      "Iteration: 1301, Training Loss: 0.0373, Training Accuracy: 0.9922\n",
      "Iteration: 1311, Training Loss: 0.0388, Training Accuracy: 0.9766\n",
      "Iteration: 1321, Training Loss: 0.1040, Training Accuracy: 0.9688\n",
      "Iteration: 1331, Training Loss: 0.0776, Training Accuracy: 0.9844\n",
      "Iteration: 1341, Training Loss: 0.1334, Training Accuracy: 0.9688\n",
      "Iteration: 1351, Training Loss: 0.1321, Training Accuracy: 0.9609\n",
      "Iteration: 1361, Training Loss: 0.1730, Training Accuracy: 0.9688\n",
      "Iteration: 1371, Training Loss: 0.0474, Training Accuracy: 0.9766\n",
      "Iteration: 1381, Training Loss: 0.0427, Training Accuracy: 0.9766\n",
      "Iteration: 1391, Training Loss: 0.2613, Training Accuracy: 0.9375\n",
      "Iteration: 1401, Training Loss: 0.1412, Training Accuracy: 0.9531\n",
      "Iteration: 1411, Training Loss: 0.2844, Training Accuracy: 0.9531\n",
      "Iteration: 1421, Training Loss: 0.0850, Training Accuracy: 0.9844\n",
      "Iteration: 1431, Training Loss: 0.0400, Training Accuracy: 0.9766\n",
      "Iteration: 1441, Training Loss: 0.0357, Training Accuracy: 0.9844\n",
      "Iteration: 1451, Training Loss: 0.0128, Training Accuracy: 1.0000\n",
      "Iteration: 1461, Training Loss: 0.1430, Training Accuracy: 0.9609\n",
      "Iteration: 1471, Training Loss: 0.0898, Training Accuracy: 0.9688\n",
      "Iteration: 1481, Training Loss: 0.1247, Training Accuracy: 0.9766\n",
      "Iteration: 1491, Training Loss: 0.0879, Training Accuracy: 0.9766\n",
      "Iteration: 1501, Training Loss: 0.1257, Training Accuracy: 0.9688\n",
      "Iteration: 1511, Training Loss: 0.0558, Training Accuracy: 0.9766\n",
      "Iteration: 1521, Training Loss: 0.0769, Training Accuracy: 0.9844\n",
      "Iteration: 1531, Training Loss: 0.1046, Training Accuracy: 0.9766\n",
      "Iteration: 1541, Training Loss: 0.0302, Training Accuracy: 0.9922\n",
      "Iteration: 1551, Training Loss: 0.0769, Training Accuracy: 0.9844\n",
      "Iteration: 1561, Training Loss: 0.0572, Training Accuracy: 0.9844\n",
      "Iteration: 1571, Training Loss: 0.0857, Training Accuracy: 0.9688\n",
      "Iteration: 1581, Training Loss: 0.1082, Training Accuracy: 0.9766\n",
      "Iteration: 1591, Training Loss: 0.0444, Training Accuracy: 0.9766\n",
      "Iteration: 1601, Training Loss: 0.2316, Training Accuracy: 0.9375\n",
      "Iteration: 1611, Training Loss: 0.0839, Training Accuracy: 0.9844\n",
      "Iteration: 1621, Training Loss: 0.1794, Training Accuracy: 0.9531\n",
      "Iteration: 1631, Training Loss: 0.0243, Training Accuracy: 0.9844\n",
      "Iteration: 1641, Training Loss: 0.0500, Training Accuracy: 0.9844\n",
      "Iteration: 1651, Training Loss: 0.1478, Training Accuracy: 0.9766\n",
      "Iteration: 1661, Training Loss: 0.1247, Training Accuracy: 0.9688\n",
      "Iteration: 1671, Training Loss: 0.0905, Training Accuracy: 0.9844\n",
      "Iteration: 1681, Training Loss: 0.0106, Training Accuracy: 1.0000\n",
      "Iteration: 1691, Training Loss: 0.0565, Training Accuracy: 0.9688\n",
      "Iteration: 1701, Training Loss: 0.1836, Training Accuracy: 0.9531\n",
      "Iteration: 1711, Training Loss: 0.0986, Training Accuracy: 0.9609\n",
      "Iteration: 1721, Training Loss: 0.0428, Training Accuracy: 0.9844\n",
      "Iteration: 1731, Training Loss: 0.0492, Training Accuracy: 0.9766\n",
      "Iteration: 1741, Training Loss: 0.0468, Training Accuracy: 0.9844\n",
      "Iteration: 1751, Training Loss: 0.1040, Training Accuracy: 0.9531\n",
      "Iteration: 1761, Training Loss: 0.2646, Training Accuracy: 0.9688\n",
      "Iteration: 1771, Training Loss: 0.1055, Training Accuracy: 0.9766\n",
      "Iteration: 1781, Training Loss: 0.1311, Training Accuracy: 0.9844\n",
      "Iteration: 1791, Training Loss: 0.1785, Training Accuracy: 0.9766\n",
      "Iteration: 1801, Training Loss: 0.0382, Training Accuracy: 0.9766\n",
      "Iteration: 1811, Training Loss: 0.0555, Training Accuracy: 0.9844\n",
      "Iteration: 1821, Training Loss: 0.1017, Training Accuracy: 0.9766\n",
      "Iteration: 1831, Training Loss: 0.0366, Training Accuracy: 0.9922\n",
      "Iteration: 1841, Training Loss: 0.0833, Training Accuracy: 0.9688\n",
      "Iteration: 1851, Training Loss: 0.0464, Training Accuracy: 0.9844\n",
      "Iteration: 1861, Training Loss: 0.0272, Training Accuracy: 0.9922\n",
      "Iteration: 1871, Training Loss: 0.1268, Training Accuracy: 0.9688\n",
      "Iteration: 1881, Training Loss: 0.0378, Training Accuracy: 0.9844\n",
      "Iteration: 1891, Training Loss: 0.0734, Training Accuracy: 0.9609\n",
      "Iteration: 1901, Training Loss: 0.0484, Training Accuracy: 0.9844\n",
      "Iteration: 1911, Training Loss: 0.0780, Training Accuracy: 0.9766\n",
      "Iteration: 1921, Training Loss: 0.0508, Training Accuracy: 0.9766\n",
      "Iteration: 1931, Training Loss: 0.0422, Training Accuracy: 0.9844\n",
      "Iteration: 1941, Training Loss: 0.1448, Training Accuracy: 0.9766\n",
      "Iteration: 1951, Training Loss: 0.0214, Training Accuracy: 0.9922\n",
      "Iteration: 1961, Training Loss: 0.1753, Training Accuracy: 0.9609\n",
      "Iteration: 1971, Training Loss: 0.0412, Training Accuracy: 0.9844\n",
      "Iteration: 1981, Training Loss: 0.0742, Training Accuracy: 0.9844\n",
      "Iteration: 1991, Training Loss: 0.0141, Training Accuracy: 1.0000\n",
      "Iteration: 2001, Training Loss: 0.0827, Training Accuracy: 0.9766\n",
      "Iteration: 2011, Training Loss: 0.1313, Training Accuracy: 0.9844\n",
      "Iteration: 2021, Training Loss: 0.1197, Training Accuracy: 0.9688\n",
      "Iteration: 2031, Training Loss: 0.0563, Training Accuracy: 0.9766\n",
      "Iteration: 2041, Training Loss: 0.1576, Training Accuracy: 0.9688\n",
      "Iteration: 2051, Training Loss: 0.0731, Training Accuracy: 0.9766\n",
      "Iteration: 2061, Training Loss: 0.1751, Training Accuracy: 0.9453\n",
      "Iteration: 2071, Training Loss: 0.0554, Training Accuracy: 0.9766\n",
      "Iteration: 2081, Training Loss: 0.2717, Training Accuracy: 0.9609\n",
      "Iteration: 2091, Training Loss: 0.1426, Training Accuracy: 0.9609\n",
      "Iteration: 2101, Training Loss: 0.0280, Training Accuracy: 0.9922\n",
      "Iteration: 2111, Training Loss: 0.0530, Training Accuracy: 0.9766\n",
      "Iteration: 2121, Training Loss: 0.0747, Training Accuracy: 0.9844\n",
      "Iteration: 2131, Training Loss: 0.0581, Training Accuracy: 0.9766\n",
      "Iteration: 2141, Training Loss: 0.0392, Training Accuracy: 0.9766\n",
      "Iteration: 2151, Training Loss: 0.0222, Training Accuracy: 0.9844\n",
      "Iteration: 2161, Training Loss: 0.0260, Training Accuracy: 0.9922\n",
      "Iteration: 2171, Training Loss: 0.0987, Training Accuracy: 0.9609\n",
      "Iteration: 2181, Training Loss: 0.0200, Training Accuracy: 0.9922\n",
      "Iteration: 2191, Training Loss: 0.1027, Training Accuracy: 0.9688\n",
      "Iteration: 2201, Training Loss: 0.0050, Training Accuracy: 1.0000\n",
      "Iteration: 2211, Training Loss: 0.0077, Training Accuracy: 0.9922\n",
      "Iteration: 2221, Training Loss: 0.0921, Training Accuracy: 0.9609\n",
      "Iteration: 2231, Training Loss: 0.0588, Training Accuracy: 0.9844\n",
      "Iteration: 2241, Training Loss: 0.0424, Training Accuracy: 0.9844\n",
      "Iteration: 2251, Training Loss: 0.0564, Training Accuracy: 0.9844\n",
      "Iteration: 2261, Training Loss: 0.0147, Training Accuracy: 0.9922\n",
      "Iteration: 2271, Training Loss: 0.0218, Training Accuracy: 0.9844\n",
      "Iteration: 2281, Training Loss: 0.1002, Training Accuracy: 0.9844\n",
      "Iteration: 2291, Training Loss: 0.0517, Training Accuracy: 0.9766\n",
      "Iteration: 2301, Training Loss: 0.0704, Training Accuracy: 0.9688\n",
      "Iteration: 2311, Training Loss: 0.1511, Training Accuracy: 0.9688\n",
      "Iteration: 2321, Training Loss: 0.0804, Training Accuracy: 0.9844\n",
      "Iteration: 2331, Training Loss: 0.2640, Training Accuracy: 0.9609\n",
      "Iteration: 2341, Training Loss: 0.1460, Training Accuracy: 0.9844\n",
      "Iteration: 2351, Training Loss: 0.0131, Training Accuracy: 1.0000\n",
      "Iteration: 2361, Training Loss: 0.2470, Training Accuracy: 0.9531\n",
      "Iteration: 2371, Training Loss: 0.1983, Training Accuracy: 0.9453\n",
      "Iteration: 2381, Training Loss: 0.1585, Training Accuracy: 0.9609\n",
      "Iteration: 2391, Training Loss: 0.0923, Training Accuracy: 0.9766\n",
      "Iteration: 2401, Training Loss: 0.0220, Training Accuracy: 0.9844\n",
      "Iteration: 2411, Training Loss: 0.1092, Training Accuracy: 0.9766\n",
      "Iteration: 2421, Training Loss: 0.1497, Training Accuracy: 0.9531\n",
      "Iteration: 2431, Training Loss: 0.0125, Training Accuracy: 1.0000\n",
      "Iteration: 2441, Training Loss: 0.0278, Training Accuracy: 0.9844\n",
      "Iteration: 2451, Training Loss: 0.0576, Training Accuracy: 0.9766\n",
      "Iteration: 2461, Training Loss: 0.1032, Training Accuracy: 0.9609\n",
      "Iteration: 2471, Training Loss: 0.0725, Training Accuracy: 0.9766\n",
      "Iteration: 2481, Training Loss: 0.0897, Training Accuracy: 0.9844\n",
      "Iteration: 2491, Training Loss: 0.0763, Training Accuracy: 0.9766\n",
      "Iteration: 2501, Training Loss: 0.0757, Training Accuracy: 0.9844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2511, Training Loss: 0.0842, Training Accuracy: 0.9688\n",
      "Iteration: 2521, Training Loss: 0.0101, Training Accuracy: 1.0000\n",
      "Iteration: 2531, Training Loss: 0.0406, Training Accuracy: 0.9922\n",
      "Iteration: 2541, Training Loss: 0.1168, Training Accuracy: 0.9688\n",
      "Iteration: 2551, Training Loss: 0.0086, Training Accuracy: 0.9922\n",
      "Iteration: 2561, Training Loss: 0.0669, Training Accuracy: 0.9766\n",
      "Iteration: 2571, Training Loss: 0.0416, Training Accuracy: 0.9844\n",
      "Iteration: 2581, Training Loss: 0.0278, Training Accuracy: 0.9922\n",
      "Iteration: 2591, Training Loss: 0.0590, Training Accuracy: 0.9766\n",
      "Iteration: 2601, Training Loss: 0.1191, Training Accuracy: 0.9453\n",
      "Iteration: 2611, Training Loss: 0.0123, Training Accuracy: 1.0000\n",
      "Iteration: 2621, Training Loss: 0.0471, Training Accuracy: 0.9766\n",
      "Iteration: 2631, Training Loss: 0.0191, Training Accuracy: 0.9922\n",
      "Iteration: 2641, Training Loss: 0.0183, Training Accuracy: 0.9922\n",
      "Iteration: 2651, Training Loss: 0.0849, Training Accuracy: 0.9688\n",
      "Iteration: 2661, Training Loss: 0.0880, Training Accuracy: 0.9766\n",
      "Iteration: 2671, Training Loss: 0.1176, Training Accuracy: 0.9688\n",
      "Iteration: 2681, Training Loss: 0.0868, Training Accuracy: 0.9844\n",
      "Iteration: 2691, Training Loss: 0.0072, Training Accuracy: 1.0000\n",
      "Iteration: 2701, Training Loss: 0.0490, Training Accuracy: 0.9844\n",
      "Iteration: 2711, Training Loss: 0.0647, Training Accuracy: 0.9922\n",
      "Iteration: 2721, Training Loss: 0.0246, Training Accuracy: 0.9922\n",
      "Iteration: 2731, Training Loss: 0.0530, Training Accuracy: 0.9844\n",
      "Iteration: 2741, Training Loss: 0.0889, Training Accuracy: 0.9688\n",
      "Iteration: 2751, Training Loss: 0.0332, Training Accuracy: 0.9922\n",
      "Iteration: 2761, Training Loss: 0.0649, Training Accuracy: 0.9922\n",
      "Iteration: 2771, Training Loss: 0.0505, Training Accuracy: 0.9766\n",
      "Iteration: 2781, Training Loss: 0.0384, Training Accuracy: 0.9844\n",
      "Iteration: 2791, Training Loss: 0.1126, Training Accuracy: 0.9453\n",
      "Iteration: 2801, Training Loss: 0.0651, Training Accuracy: 0.9766\n",
      "Iteration: 2811, Training Loss: 0.1150, Training Accuracy: 0.9766\n",
      "Iteration: 2821, Training Loss: 0.0429, Training Accuracy: 0.9844\n",
      "Iteration: 2831, Training Loss: 0.0670, Training Accuracy: 0.9844\n",
      "Iteration: 2841, Training Loss: 0.0403, Training Accuracy: 0.9766\n",
      "Iteration: 2851, Training Loss: 0.0602, Training Accuracy: 0.9844\n",
      "Iteration: 2861, Training Loss: 0.0302, Training Accuracy: 0.9922\n",
      "Iteration: 2871, Training Loss: 0.0112, Training Accuracy: 1.0000\n",
      "Iteration: 2881, Training Loss: 0.0521, Training Accuracy: 0.9844\n",
      "Iteration: 2891, Training Loss: 0.1146, Training Accuracy: 0.9688\n",
      "Iteration: 2901, Training Loss: 0.0731, Training Accuracy: 0.9766\n",
      "Iteration: 2911, Training Loss: 0.1359, Training Accuracy: 0.9766\n",
      "Iteration: 2921, Training Loss: 0.0892, Training Accuracy: 0.9844\n",
      "Iteration: 2931, Training Loss: 0.0665, Training Accuracy: 0.9688\n",
      "Iteration: 2941, Training Loss: 0.0331, Training Accuracy: 0.9922\n",
      "Iteration: 2951, Training Loss: 0.1367, Training Accuracy: 0.9688\n",
      "Iteration: 2961, Training Loss: 0.2046, Training Accuracy: 0.9844\n",
      "Iteration: 2971, Training Loss: 0.0247, Training Accuracy: 0.9922\n",
      "Iteration: 2981, Training Loss: 0.1150, Training Accuracy: 0.9766\n",
      "Iteration: 2991, Training Loss: 0.1155, Training Accuracy: 0.9609\n",
      "Iteration: 3001, Training Loss: 0.1537, Training Accuracy: 0.9531\n",
      "Iteration: 3011, Training Loss: 0.0771, Training Accuracy: 0.9766\n",
      "Iteration: 3021, Training Loss: 0.0594, Training Accuracy: 0.9844\n",
      "Iteration: 3031, Training Loss: 0.0380, Training Accuracy: 0.9844\n",
      "Iteration: 3041, Training Loss: 0.1119, Training Accuracy: 0.9766\n",
      "Iteration: 3051, Training Loss: 0.1612, Training Accuracy: 0.9766\n",
      "Iteration: 3061, Training Loss: 0.0293, Training Accuracy: 0.9922\n",
      "Iteration: 3071, Training Loss: 0.0478, Training Accuracy: 0.9922\n",
      "Iteration: 3081, Training Loss: 0.0103, Training Accuracy: 1.0000\n",
      "Iteration: 3091, Training Loss: 0.1644, Training Accuracy: 0.9688\n",
      "Iteration: 3101, Training Loss: 0.0304, Training Accuracy: 0.9922\n",
      "Iteration: 3111, Training Loss: 0.0883, Training Accuracy: 0.9844\n",
      "Iteration: 3121, Training Loss: 0.0473, Training Accuracy: 0.9844\n",
      "Iteration: 3131, Training Loss: 0.0653, Training Accuracy: 0.9844\n",
      "Iteration: 3141, Training Loss: 0.0527, Training Accuracy: 0.9844\n",
      "Iteration: 3151, Training Loss: 0.1002, Training Accuracy: 0.9609\n",
      "Iteration: 3161, Training Loss: 0.0891, Training Accuracy: 0.9609\n",
      "Iteration: 3171, Training Loss: 0.1033, Training Accuracy: 0.9688\n",
      "Iteration: 3181, Training Loss: 0.0129, Training Accuracy: 0.9922\n",
      "Iteration: 3191, Training Loss: 0.0240, Training Accuracy: 0.9844\n",
      "Iteration: 3201, Training Loss: 0.0141, Training Accuracy: 0.9922\n",
      "Iteration: 3211, Training Loss: 0.0169, Training Accuracy: 0.9922\n",
      "Iteration: 3221, Training Loss: 0.0245, Training Accuracy: 0.9844\n",
      "Iteration: 3231, Training Loss: 0.0719, Training Accuracy: 0.9766\n",
      "Iteration: 3241, Training Loss: 0.0928, Training Accuracy: 0.9609\n",
      "Iteration: 3251, Training Loss: 0.0559, Training Accuracy: 0.9688\n",
      "Iteration: 3261, Training Loss: 0.0574, Training Accuracy: 0.9844\n",
      "Iteration: 3271, Training Loss: 0.1290, Training Accuracy: 0.9688\n",
      "Iteration: 3281, Training Loss: 0.0519, Training Accuracy: 0.9766\n",
      "Iteration: 3291, Training Loss: 0.0622, Training Accuracy: 0.9766\n",
      "Iteration: 3301, Training Loss: 0.0174, Training Accuracy: 0.9922\n",
      "Iteration: 3311, Training Loss: 0.0863, Training Accuracy: 0.9766\n",
      "Iteration: 3321, Training Loss: 0.0255, Training Accuracy: 0.9922\n",
      "Iteration: 3331, Training Loss: 0.0901, Training Accuracy: 0.9766\n",
      "Iteration: 3341, Training Loss: 0.1414, Training Accuracy: 0.9688\n",
      "Iteration: 3351, Training Loss: 0.0936, Training Accuracy: 0.9844\n",
      "Iteration: 3361, Training Loss: 0.0206, Training Accuracy: 1.0000\n",
      "Iteration: 3371, Training Loss: 0.1672, Training Accuracy: 0.9688\n",
      "Iteration: 3381, Training Loss: 0.0203, Training Accuracy: 0.9922\n",
      "Iteration: 3391, Training Loss: 0.1218, Training Accuracy: 0.9609\n",
      "Iteration: 3401, Training Loss: 0.1177, Training Accuracy: 0.9688\n",
      "Iteration: 3411, Training Loss: 0.0958, Training Accuracy: 0.9844\n",
      "Iteration: 3421, Training Loss: 0.1488, Training Accuracy: 0.9844\n",
      "Iteration: 3431, Training Loss: 0.1868, Training Accuracy: 0.9609\n",
      "Iteration: 3441, Training Loss: 0.0911, Training Accuracy: 0.9609\n",
      "Iteration: 3451, Training Loss: 0.1156, Training Accuracy: 0.9531\n",
      "Iteration: 3461, Training Loss: 0.0301, Training Accuracy: 0.9844\n",
      "Iteration: 3471, Training Loss: 0.0298, Training Accuracy: 0.9844\n",
      "Iteration: 3481, Training Loss: 0.1044, Training Accuracy: 0.9844\n",
      "Iteration: 3491, Training Loss: 0.0274, Training Accuracy: 0.9922\n",
      "Iteration: 3501, Training Loss: 0.2639, Training Accuracy: 0.9531\n",
      "Iteration: 3511, Training Loss: 0.1489, Training Accuracy: 0.9531\n",
      "Iteration: 3521, Training Loss: 0.1834, Training Accuracy: 0.9609\n",
      "Iteration: 3531, Training Loss: 0.0080, Training Accuracy: 1.0000\n",
      "Iteration: 3541, Training Loss: 0.1221, Training Accuracy: 0.9766\n",
      "Iteration: 3551, Training Loss: 0.1845, Training Accuracy: 0.9531\n",
      "Iteration: 3561, Training Loss: 0.2233, Training Accuracy: 0.9453\n",
      "Iteration: 3571, Training Loss: 0.1664, Training Accuracy: 0.9453\n",
      "Iteration: 3581, Training Loss: 0.1470, Training Accuracy: 0.9531\n",
      "Iteration: 3591, Training Loss: 0.0860, Training Accuracy: 0.9766\n",
      "Iteration: 3601, Training Loss: 0.0231, Training Accuracy: 0.9922\n",
      "Iteration: 3611, Training Loss: 0.0580, Training Accuracy: 0.9766\n",
      "Iteration: 3621, Training Loss: 0.0766, Training Accuracy: 0.9766\n",
      "Iteration: 3631, Training Loss: 0.1291, Training Accuracy: 0.9609\n",
      "Iteration: 3641, Training Loss: 0.1173, Training Accuracy: 0.9766\n",
      "Iteration: 3651, Training Loss: 0.2007, Training Accuracy: 0.9531\n",
      "Iteration: 3661, Training Loss: 0.0519, Training Accuracy: 0.9766\n",
      "Iteration: 3671, Training Loss: 0.0519, Training Accuracy: 0.9844\n",
      "Iteration: 3681, Training Loss: 0.2818, Training Accuracy: 0.9531\n",
      "Iteration: 3691, Training Loss: 0.2081, Training Accuracy: 0.9375\n",
      "Iteration: 3701, Training Loss: 0.0751, Training Accuracy: 0.9844\n",
      "Iteration: 3711, Training Loss: 0.0263, Training Accuracy: 0.9844\n",
      "Iteration: 3721, Training Loss: 0.1461, Training Accuracy: 0.9688\n",
      "Iteration: 3731, Training Loss: 0.0634, Training Accuracy: 0.9922\n",
      "Iteration: 3741, Training Loss: 0.0451, Training Accuracy: 0.9844\n",
      "Iteration: 3751, Training Loss: 0.1731, Training Accuracy: 0.9688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3761, Training Loss: 0.0935, Training Accuracy: 0.9766\n",
      "Iteration: 3771, Training Loss: 0.0544, Training Accuracy: 0.9844\n",
      "Iteration: 3781, Training Loss: 0.1145, Training Accuracy: 0.9531\n",
      "Iteration: 3791, Training Loss: 0.1479, Training Accuracy: 0.9531\n",
      "Iteration: 3801, Training Loss: 0.2045, Training Accuracy: 0.9453\n",
      "Iteration: 3811, Training Loss: 0.1556, Training Accuracy: 0.9531\n",
      "Iteration: 3821, Training Loss: 0.0332, Training Accuracy: 0.9922\n",
      "Iteration: 3831, Training Loss: 0.1672, Training Accuracy: 0.9844\n",
      "Iteration: 3841, Training Loss: 0.0503, Training Accuracy: 0.9766\n",
      "Iteration: 3851, Training Loss: 0.1071, Training Accuracy: 0.9844\n",
      "Iteration: 3861, Training Loss: 0.0706, Training Accuracy: 0.9688\n",
      "Iteration: 3871, Training Loss: 0.2384, Training Accuracy: 0.9375\n",
      "Iteration: 3881, Training Loss: 0.2501, Training Accuracy: 0.9609\n",
      "Iteration: 3891, Training Loss: 0.0666, Training Accuracy: 0.9766\n",
      "Iteration: 3901, Training Loss: 0.0722, Training Accuracy: 0.9766\n",
      "Iteration: 3911, Training Loss: 0.0146, Training Accuracy: 0.9922\n",
      "Iteration: 3921, Training Loss: 0.0979, Training Accuracy: 0.9609\n",
      "Iteration: 3931, Training Loss: 0.0804, Training Accuracy: 0.9844\n",
      "Iteration: 3941, Training Loss: 0.0861, Training Accuracy: 0.9766\n",
      "Iteration: 3951, Training Loss: 0.0371, Training Accuracy: 0.9922\n",
      "Iteration: 3961, Training Loss: 0.2566, Training Accuracy: 0.9609\n",
      "Iteration: 3971, Training Loss: 0.0211, Training Accuracy: 0.9922\n",
      "Iteration: 3981, Training Loss: 0.1134, Training Accuracy: 0.9766\n",
      "Iteration: 3991, Training Loss: 0.1811, Training Accuracy: 0.9609\n",
      "Iteration: 4001, Training Loss: 0.0787, Training Accuracy: 0.9688\n",
      "Iteration: 4011, Training Loss: 0.0053, Training Accuracy: 1.0000\n",
      "Iteration: 4021, Training Loss: 0.0690, Training Accuracy: 0.9844\n",
      "Iteration: 4031, Training Loss: 0.0455, Training Accuracy: 0.9844\n",
      "Iteration: 4041, Training Loss: 0.1060, Training Accuracy: 0.9766\n",
      "Iteration: 4051, Training Loss: 0.0608, Training Accuracy: 0.9766\n",
      "Iteration: 4061, Training Loss: 0.0378, Training Accuracy: 0.9844\n",
      "Iteration: 4071, Training Loss: 0.0717, Training Accuracy: 0.9609\n",
      "Iteration: 4081, Training Loss: 0.1606, Training Accuracy: 0.9688\n",
      "Iteration: 4091, Training Loss: 0.2152, Training Accuracy: 0.9453\n",
      "Iteration: 4101, Training Loss: 0.1587, Training Accuracy: 0.9688\n",
      "Iteration: 4111, Training Loss: 0.1877, Training Accuracy: 0.9766\n",
      "Iteration: 4121, Training Loss: 0.1156, Training Accuracy: 0.9766\n",
      "Iteration: 4131, Training Loss: 0.2117, Training Accuracy: 0.9922\n",
      "Iteration: 4141, Training Loss: 0.1364, Training Accuracy: 0.9766\n",
      "Iteration: 4151, Training Loss: 0.0154, Training Accuracy: 1.0000\n",
      "Iteration: 4161, Training Loss: 0.1246, Training Accuracy: 0.9844\n",
      "Iteration: 4171, Training Loss: 0.0879, Training Accuracy: 0.9766\n",
      "Iteration: 4181, Training Loss: 0.0853, Training Accuracy: 0.9922\n",
      "Iteration: 4191, Training Loss: 0.1947, Training Accuracy: 0.9609\n",
      "Iteration: 4201, Training Loss: 0.0026, Training Accuracy: 1.0000\n",
      "Iteration: 4211, Training Loss: 0.1817, Training Accuracy: 0.9609\n",
      "Iteration: 4221, Training Loss: 0.0341, Training Accuracy: 0.9922\n",
      "Iteration: 4231, Training Loss: 0.0730, Training Accuracy: 0.9844\n",
      "Iteration: 4241, Training Loss: 0.2287, Training Accuracy: 0.9453\n",
      "Iteration: 4251, Training Loss: 0.1646, Training Accuracy: 0.9766\n",
      "Iteration: 4261, Training Loss: 0.0218, Training Accuracy: 0.9922\n",
      "Iteration: 4271, Training Loss: 0.1720, Training Accuracy: 0.9688\n",
      "Iteration: 4281, Training Loss: 0.0718, Training Accuracy: 0.9766\n",
      "Iteration: 4291, Training Loss: 0.1630, Training Accuracy: 0.9531\n",
      "Iteration: 4301, Training Loss: 0.0325, Training Accuracy: 0.9844\n",
      "Iteration: 4311, Training Loss: 0.0653, Training Accuracy: 0.9922\n",
      "Iteration: 4321, Training Loss: 0.0614, Training Accuracy: 0.9922\n",
      "Iteration: 4331, Training Loss: 0.0431, Training Accuracy: 0.9844\n",
      "Iteration: 4341, Training Loss: 0.0853, Training Accuracy: 0.9766\n",
      "Iteration: 4351, Training Loss: 0.1752, Training Accuracy: 0.9609\n",
      "Iteration: 4361, Training Loss: 0.1398, Training Accuracy: 0.9688\n",
      "Iteration: 4371, Training Loss: 0.1666, Training Accuracy: 0.9531\n",
      "Iteration: 4381, Training Loss: 0.1759, Training Accuracy: 0.9688\n",
      "Iteration: 4391, Training Loss: 0.0711, Training Accuracy: 0.9844\n",
      "Iteration: 4401, Training Loss: 0.2278, Training Accuracy: 0.9766\n",
      "Iteration: 4411, Training Loss: 0.1111, Training Accuracy: 0.9766\n",
      "Iteration: 4421, Training Loss: 0.0585, Training Accuracy: 0.9922\n",
      "Iteration: 4431, Training Loss: 0.0164, Training Accuracy: 0.9922\n",
      "Iteration: 4441, Training Loss: 0.1170, Training Accuracy: 0.9688\n",
      "Iteration: 4451, Training Loss: 0.2048, Training Accuracy: 0.9609\n",
      "Iteration: 4461, Training Loss: 0.1120, Training Accuracy: 0.9688\n",
      "Iteration: 4471, Training Loss: 0.0791, Training Accuracy: 0.9844\n",
      "Iteration: 4481, Training Loss: 0.2624, Training Accuracy: 0.9766\n",
      "Iteration: 4491, Training Loss: 0.2525, Training Accuracy: 0.9453\n",
      "Iteration: 4501, Training Loss: 0.1389, Training Accuracy: 0.9688\n",
      "Iteration: 4511, Training Loss: 0.1337, Training Accuracy: 0.9609\n",
      "Iteration: 4521, Training Loss: 0.0358, Training Accuracy: 0.9922\n",
      "Iteration: 4531, Training Loss: 0.2491, Training Accuracy: 0.9609\n",
      "Iteration: 4541, Training Loss: 0.0660, Training Accuracy: 0.9844\n",
      "Iteration: 4551, Training Loss: 0.1121, Training Accuracy: 0.9609\n",
      "Iteration: 4561, Training Loss: 0.1666, Training Accuracy: 0.9609\n",
      "Iteration: 4571, Training Loss: 0.0303, Training Accuracy: 0.9844\n",
      "Iteration: 4581, Training Loss: 0.0625, Training Accuracy: 0.9766\n",
      "Iteration: 4591, Training Loss: 0.1582, Training Accuracy: 0.9688\n",
      "Iteration: 4601, Training Loss: 0.0585, Training Accuracy: 0.9688\n",
      "Iteration: 4611, Training Loss: 0.1867, Training Accuracy: 0.9531\n",
      "Iteration: 4621, Training Loss: 0.0465, Training Accuracy: 0.9922\n",
      "Iteration: 4631, Training Loss: 0.0052, Training Accuracy: 1.0000\n",
      "Iteration: 4641, Training Loss: 0.0074, Training Accuracy: 1.0000\n",
      "Iteration: 4651, Training Loss: 0.0141, Training Accuracy: 1.0000\n",
      "Iteration: 4661, Training Loss: 0.0576, Training Accuracy: 0.9766\n",
      "Iteration: 4671, Training Loss: 0.2553, Training Accuracy: 0.9766\n",
      "Iteration: 4681, Training Loss: 0.2011, Training Accuracy: 0.9453\n",
      "Iteration: 4691, Training Loss: 0.1315, Training Accuracy: 0.9609\n",
      "Iteration: 4701, Training Loss: 0.1497, Training Accuracy: 0.9531\n",
      "Iteration: 4711, Training Loss: 0.1937, Training Accuracy: 0.9531\n",
      "Iteration: 4721, Training Loss: 0.2554, Training Accuracy: 0.9531\n",
      "Iteration: 4731, Training Loss: 0.0681, Training Accuracy: 0.9766\n",
      "Iteration: 4741, Training Loss: 0.0905, Training Accuracy: 0.9766\n",
      "Iteration: 4751, Training Loss: 0.1195, Training Accuracy: 0.9609\n",
      "Iteration: 4761, Training Loss: 0.0229, Training Accuracy: 0.9922\n",
      "Iteration: 4771, Training Loss: 0.1583, Training Accuracy: 0.9766\n",
      "Iteration: 4781, Training Loss: 0.1062, Training Accuracy: 0.9609\n",
      "Iteration: 4791, Training Loss: 0.2839, Training Accuracy: 0.9453\n",
      "Iteration: 4801, Training Loss: 0.1235, Training Accuracy: 0.9922\n",
      "Iteration: 4811, Training Loss: 0.3399, Training Accuracy: 0.9531\n",
      "Iteration: 4821, Training Loss: 0.0677, Training Accuracy: 0.9688\n",
      "Iteration: 4831, Training Loss: 0.1606, Training Accuracy: 0.9766\n",
      "Iteration: 4841, Training Loss: 0.1515, Training Accuracy: 0.9844\n",
      "Iteration: 4851, Training Loss: 0.0851, Training Accuracy: 0.9766\n",
      "Iteration: 4861, Training Loss: 0.2959, Training Accuracy: 0.9531\n",
      "Iteration: 4871, Training Loss: 0.1364, Training Accuracy: 0.9766\n",
      "Iteration: 4881, Training Loss: 0.1307, Training Accuracy: 0.9688\n",
      "Iteration: 4891, Training Loss: 0.2715, Training Accuracy: 0.9609\n",
      "Iteration: 4901, Training Loss: 0.1217, Training Accuracy: 0.9688\n",
      "Iteration: 4911, Training Loss: 0.0659, Training Accuracy: 0.9766\n",
      "Iteration: 4921, Training Loss: 0.0552, Training Accuracy: 0.9766\n",
      "Iteration: 4931, Training Loss: 0.1254, Training Accuracy: 0.9688\n",
      "Iteration: 4941, Training Loss: 0.0754, Training Accuracy: 0.9922\n",
      "Iteration: 4951, Training Loss: 0.0637, Training Accuracy: 0.9688\n",
      "Iteration: 4961, Training Loss: 0.0355, Training Accuracy: 0.9844\n",
      "Iteration: 4971, Training Loss: 0.0374, Training Accuracy: 0.9844\n",
      "Iteration: 4981, Training Loss: 0.0401, Training Accuracy: 0.9844\n",
      "Iteration: 4991, Training Loss: 0.0734, Training Accuracy: 0.9844\n",
      "Iteration: 5001, Training Loss: 0.1630, Training Accuracy: 0.9688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5011, Training Loss: 0.0653, Training Accuracy: 0.9766\n",
      "Iteration: 5021, Training Loss: 0.0021, Training Accuracy: 1.0000\n",
      "Iteration: 5031, Training Loss: 0.1274, Training Accuracy: 0.9766\n",
      "Iteration: 5041, Training Loss: 0.0227, Training Accuracy: 0.9922\n",
      "Iteration: 5051, Training Loss: 0.2061, Training Accuracy: 0.9531\n",
      "Iteration: 5061, Training Loss: 0.1204, Training Accuracy: 0.9766\n",
      "Iteration: 5071, Training Loss: 0.0555, Training Accuracy: 0.9922\n",
      "Iteration: 5081, Training Loss: 0.0984, Training Accuracy: 0.9766\n",
      "Iteration: 5091, Training Loss: 0.0934, Training Accuracy: 0.9766\n",
      "Iteration: 5101, Training Loss: 0.2916, Training Accuracy: 0.9609\n",
      "Iteration: 5111, Training Loss: 0.1496, Training Accuracy: 0.9453\n",
      "Iteration: 5121, Training Loss: 0.0516, Training Accuracy: 0.9844\n",
      "Iteration: 5131, Training Loss: 0.2164, Training Accuracy: 0.9688\n",
      "Iteration: 5141, Training Loss: 0.0434, Training Accuracy: 0.9922\n",
      "Iteration: 5151, Training Loss: 0.0654, Training Accuracy: 0.9844\n",
      "Iteration: 5161, Training Loss: 0.0179, Training Accuracy: 1.0000\n",
      "Iteration: 5171, Training Loss: 0.1364, Training Accuracy: 0.9531\n",
      "Iteration: 5181, Training Loss: 0.1008, Training Accuracy: 0.9844\n",
      "Iteration: 5191, Training Loss: 0.1131, Training Accuracy: 0.9766\n",
      "Iteration: 5201, Training Loss: 0.0511, Training Accuracy: 0.9844\n",
      "Iteration: 5211, Training Loss: 0.2529, Training Accuracy: 0.9766\n",
      "Iteration: 5221, Training Loss: 0.0395, Training Accuracy: 0.9844\n",
      "Iteration: 5231, Training Loss: 0.0726, Training Accuracy: 0.9766\n",
      "Iteration: 5241, Training Loss: 0.0904, Training Accuracy: 0.9766\n",
      "Iteration: 5251, Training Loss: 0.0909, Training Accuracy: 0.9922\n",
      "Iteration: 5261, Training Loss: 0.1634, Training Accuracy: 0.9766\n",
      "Iteration: 5271, Training Loss: 0.0616, Training Accuracy: 0.9766\n",
      "Iteration: 5281, Training Loss: 0.1905, Training Accuracy: 0.9844\n",
      "Iteration: 5291, Training Loss: 0.1413, Training Accuracy: 0.9688\n",
      "Iteration: 5301, Training Loss: 0.1949, Training Accuracy: 0.9766\n",
      "Iteration: 5311, Training Loss: 0.1454, Training Accuracy: 0.9766\n",
      "Iteration: 5321, Training Loss: 0.1692, Training Accuracy: 0.9688\n",
      "Iteration: 5331, Training Loss: 0.0830, Training Accuracy: 0.9844\n",
      "Iteration: 5341, Training Loss: 0.0935, Training Accuracy: 0.9688\n",
      "Iteration: 5351, Training Loss: 0.0353, Training Accuracy: 0.9844\n",
      "Iteration: 5361, Training Loss: 0.2060, Training Accuracy: 0.9688\n",
      "Iteration: 5371, Training Loss: 0.0959, Training Accuracy: 0.9609\n",
      "Iteration: 5381, Training Loss: 0.1357, Training Accuracy: 0.9844\n",
      "Iteration: 5391, Training Loss: 0.0837, Training Accuracy: 0.9844\n",
      "Iteration: 5401, Training Loss: 0.0943, Training Accuracy: 0.9922\n",
      "Iteration: 5411, Training Loss: 0.1261, Training Accuracy: 0.9766\n",
      "Iteration: 5421, Training Loss: 0.1129, Training Accuracy: 0.9766\n",
      "Iteration: 5431, Training Loss: 0.0924, Training Accuracy: 0.9844\n",
      "Iteration: 5441, Training Loss: 0.0954, Training Accuracy: 0.9844\n",
      "Iteration: 5451, Training Loss: 0.0457, Training Accuracy: 0.9844\n",
      "Iteration: 5461, Training Loss: 0.0058, Training Accuracy: 1.0000\n",
      "Iteration: 5471, Training Loss: 0.2293, Training Accuracy: 0.9219\n",
      "Iteration: 5481, Training Loss: 0.2200, Training Accuracy: 0.9453\n",
      "Iteration: 5491, Training Loss: 0.0557, Training Accuracy: 0.9766\n",
      "Iteration: 5501, Training Loss: 0.0380, Training Accuracy: 0.9766\n",
      "Iteration: 5511, Training Loss: 0.1095, Training Accuracy: 0.9766\n",
      "Iteration: 5521, Training Loss: 0.2213, Training Accuracy: 0.9766\n",
      "Iteration: 5531, Training Loss: 0.0510, Training Accuracy: 0.9844\n",
      "Iteration: 5541, Training Loss: 0.2716, Training Accuracy: 0.9375\n",
      "Iteration: 5551, Training Loss: 0.2382, Training Accuracy: 0.9766\n",
      "Iteration: 5561, Training Loss: 0.1105, Training Accuracy: 0.9688\n",
      "Iteration: 5571, Training Loss: 0.1469, Training Accuracy: 0.9844\n",
      "Iteration: 5581, Training Loss: 0.1149, Training Accuracy: 0.9766\n",
      "Iteration: 5591, Training Loss: 0.0712, Training Accuracy: 0.9766\n",
      "Iteration: 5601, Training Loss: 0.0797, Training Accuracy: 0.9844\n",
      "Iteration: 5611, Training Loss: 0.1654, Training Accuracy: 0.9453\n",
      "Iteration: 5621, Training Loss: 0.1986, Training Accuracy: 0.9531\n",
      "Iteration: 5631, Training Loss: 0.0804, Training Accuracy: 0.9688\n",
      "Iteration: 5641, Training Loss: 0.1306, Training Accuracy: 0.9531\n",
      "Iteration: 5651, Training Loss: 0.0959, Training Accuracy: 0.9844\n",
      "Iteration: 5661, Training Loss: 0.1574, Training Accuracy: 0.9844\n",
      "Iteration: 5671, Training Loss: 0.0856, Training Accuracy: 0.9844\n",
      "Iteration: 5681, Training Loss: 0.1621, Training Accuracy: 0.9688\n",
      "Iteration: 5691, Training Loss: 0.1214, Training Accuracy: 0.9766\n",
      "Iteration: 5701, Training Loss: 0.1735, Training Accuracy: 0.9688\n",
      "Iteration: 5711, Training Loss: 0.0436, Training Accuracy: 0.9844\n",
      "Iteration: 5721, Training Loss: 0.2122, Training Accuracy: 0.9766\n",
      "Iteration: 5731, Training Loss: 0.1261, Training Accuracy: 0.9688\n",
      "Iteration: 5741, Training Loss: 0.0589, Training Accuracy: 0.9844\n",
      "Iteration: 5751, Training Loss: 0.2427, Training Accuracy: 0.9609\n",
      "Iteration: 5761, Training Loss: 0.1357, Training Accuracy: 0.9844\n",
      "Iteration: 5771, Training Loss: 0.0168, Training Accuracy: 0.9844\n",
      "Iteration: 5781, Training Loss: 0.1301, Training Accuracy: 0.9766\n",
      "Iteration: 5791, Training Loss: 0.2840, Training Accuracy: 0.9688\n",
      "Iteration: 5801, Training Loss: 0.1746, Training Accuracy: 0.9531\n",
      "Iteration: 5811, Training Loss: 0.2852, Training Accuracy: 0.9453\n",
      "Iteration: 5821, Training Loss: 0.8064, Training Accuracy: 0.8672\n",
      "Iteration: 5831, Training Loss: 0.3148, Training Accuracy: 0.9531\n",
      "Iteration: 5841, Training Loss: 0.2244, Training Accuracy: 0.9531\n",
      "Iteration: 5851, Training Loss: 0.2132, Training Accuracy: 0.9609\n",
      "Iteration: 5861, Training Loss: 0.2587, Training Accuracy: 0.9531\n",
      "Iteration: 5871, Training Loss: 0.2416, Training Accuracy: 0.9531\n",
      "Iteration: 5881, Training Loss: 0.4287, Training Accuracy: 0.9609\n",
      "Iteration: 5891, Training Loss: 0.1624, Training Accuracy: 0.9453\n",
      "Iteration: 5901, Training Loss: 0.1295, Training Accuracy: 0.9766\n",
      "Iteration: 5911, Training Loss: 0.0424, Training Accuracy: 0.9922\n",
      "Iteration: 5921, Training Loss: 0.3342, Training Accuracy: 0.9531\n",
      "Iteration: 5931, Training Loss: 0.1097, Training Accuracy: 0.9609\n",
      "Iteration: 5941, Training Loss: 0.1069, Training Accuracy: 0.9609\n",
      "Iteration: 5951, Training Loss: 0.1925, Training Accuracy: 0.9453\n",
      "Iteration: 5961, Training Loss: 0.2124, Training Accuracy: 0.9609\n",
      "Iteration: 5971, Training Loss: 0.0787, Training Accuracy: 0.9844\n",
      "Iteration: 5981, Training Loss: 0.0826, Training Accuracy: 0.9766\n",
      "Iteration: 5991, Training Loss: 0.0574, Training Accuracy: 0.9766\n",
      "Iteration: 6001, Training Loss: 0.1103, Training Accuracy: 0.9688\n",
      "Iteration: 6011, Training Loss: 0.3206, Training Accuracy: 0.9453\n",
      "Iteration: 6021, Training Loss: 0.1170, Training Accuracy: 0.9766\n",
      "Iteration: 6031, Training Loss: 0.0256, Training Accuracy: 0.9922\n",
      "Iteration: 6041, Training Loss: 0.2823, Training Accuracy: 0.9844\n",
      "Iteration: 6051, Training Loss: 0.0859, Training Accuracy: 0.9922\n",
      "Iteration: 6061, Training Loss: 0.0899, Training Accuracy: 0.9688\n",
      "Iteration: 6071, Training Loss: 0.0647, Training Accuracy: 0.9766\n",
      "Iteration: 6081, Training Loss: 0.0756, Training Accuracy: 0.9688\n",
      "Iteration: 6091, Training Loss: 0.0533, Training Accuracy: 0.9844\n",
      "Iteration: 6101, Training Loss: 0.0822, Training Accuracy: 0.9688\n",
      "Iteration: 6111, Training Loss: 0.0400, Training Accuracy: 0.9844\n",
      "Iteration: 6121, Training Loss: 0.0855, Training Accuracy: 0.9766\n",
      "Iteration: 6131, Training Loss: 0.0105, Training Accuracy: 0.9922\n",
      "Iteration: 6141, Training Loss: 0.0329, Training Accuracy: 0.9922\n",
      "Iteration: 6151, Training Loss: 0.1249, Training Accuracy: 0.9766\n",
      "Iteration: 6161, Training Loss: 0.1985, Training Accuracy: 0.9609\n",
      "Iteration: 6171, Training Loss: 0.1721, Training Accuracy: 0.9609\n",
      "Iteration: 6181, Training Loss: 0.1879, Training Accuracy: 0.9531\n",
      "Iteration: 6191, Training Loss: 0.1309, Training Accuracy: 0.9844\n",
      "Iteration: 6201, Training Loss: 0.0203, Training Accuracy: 0.9844\n",
      "Iteration: 6211, Training Loss: 0.1237, Training Accuracy: 0.9688\n",
      "Iteration: 6221, Training Loss: 0.3139, Training Accuracy: 0.9609\n",
      "Iteration: 6231, Training Loss: 0.0929, Training Accuracy: 0.9844\n",
      "Iteration: 6241, Training Loss: 0.0179, Training Accuracy: 0.9922\n",
      "Iteration: 6251, Training Loss: 0.1035, Training Accuracy: 0.9688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6261, Training Loss: 0.1743, Training Accuracy: 0.9141\n",
      "Iteration: 6271, Training Loss: 0.3209, Training Accuracy: 0.9297\n",
      "Iteration: 6281, Training Loss: 0.0748, Training Accuracy: 0.9609\n",
      "Iteration: 6291, Training Loss: 0.1201, Training Accuracy: 0.9766\n",
      "Iteration: 6301, Training Loss: 0.1418, Training Accuracy: 0.9609\n",
      "Iteration: 6311, Training Loss: 0.0392, Training Accuracy: 0.9766\n",
      "Iteration: 6321, Training Loss: 0.1369, Training Accuracy: 0.9688\n",
      "Iteration: 6331, Training Loss: 0.1074, Training Accuracy: 0.9609\n",
      "Iteration: 6341, Training Loss: 0.0981, Training Accuracy: 0.9531\n",
      "Iteration: 6351, Training Loss: 0.1385, Training Accuracy: 0.9688\n",
      "Iteration: 6361, Training Loss: 0.1413, Training Accuracy: 0.9766\n",
      "Iteration: 6371, Training Loss: 0.0868, Training Accuracy: 0.9609\n",
      "Iteration: 6381, Training Loss: 0.0716, Training Accuracy: 0.9844\n",
      "Iteration: 6391, Training Loss: 0.0603, Training Accuracy: 0.9766\n",
      "Iteration: 6401, Training Loss: 0.0842, Training Accuracy: 0.9609\n",
      "Iteration: 6411, Training Loss: 0.1781, Training Accuracy: 0.9531\n",
      "Iteration: 6421, Training Loss: 0.1094, Training Accuracy: 0.9688\n",
      "Iteration: 6431, Training Loss: 0.0564, Training Accuracy: 0.9844\n",
      "Iteration: 6441, Training Loss: 0.0877, Training Accuracy: 0.9688\n",
      "Iteration: 6451, Training Loss: 0.0907, Training Accuracy: 0.9531\n",
      "Iteration: 6461, Training Loss: 0.2885, Training Accuracy: 0.9766\n",
      "Iteration: 6471, Training Loss: 0.2469, Training Accuracy: 0.9609\n",
      "Iteration: 6481, Training Loss: 0.0811, Training Accuracy: 0.9766\n",
      "Iteration: 6491, Training Loss: 0.1615, Training Accuracy: 0.9844\n",
      "Iteration: 6501, Training Loss: 0.1992, Training Accuracy: 0.9609\n",
      "Iteration: 6511, Training Loss: 0.2954, Training Accuracy: 0.9531\n",
      "Iteration: 6521, Training Loss: 0.0300, Training Accuracy: 0.9844\n",
      "Iteration: 6531, Training Loss: 0.0206, Training Accuracy: 0.9922\n",
      "Iteration: 6541, Training Loss: 0.7283, Training Accuracy: 0.9531\n",
      "Iteration: 6551, Training Loss: 0.0513, Training Accuracy: 0.9766\n",
      "Iteration: 6561, Training Loss: 0.0549, Training Accuracy: 0.9766\n",
      "Iteration: 6571, Training Loss: 0.1239, Training Accuracy: 0.9531\n",
      "Iteration: 6581, Training Loss: 0.0447, Training Accuracy: 0.9766\n",
      "Iteration: 6591, Training Loss: 0.0649, Training Accuracy: 0.9844\n",
      "Iteration: 6601, Training Loss: 0.2506, Training Accuracy: 0.9531\n",
      "Iteration: 6611, Training Loss: 0.0731, Training Accuracy: 0.9766\n",
      "Iteration: 6621, Training Loss: 0.1406, Training Accuracy: 0.9609\n",
      "Iteration: 6631, Training Loss: 0.0641, Training Accuracy: 0.9766\n",
      "Iteration: 6641, Training Loss: 0.1302, Training Accuracy: 0.9609\n",
      "Iteration: 6651, Training Loss: 0.0385, Training Accuracy: 0.9844\n",
      "Iteration: 6661, Training Loss: 0.1937, Training Accuracy: 0.9688\n",
      "Iteration: 6671, Training Loss: 0.1123, Training Accuracy: 0.9609\n",
      "Iteration: 6681, Training Loss: 0.1500, Training Accuracy: 0.9688\n",
      "Iteration: 6691, Training Loss: 0.1443, Training Accuracy: 0.9609\n",
      "Iteration: 6701, Training Loss: 0.1129, Training Accuracy: 0.9688\n",
      "Iteration: 6711, Training Loss: 0.2499, Training Accuracy: 0.9609\n",
      "Iteration: 6721, Training Loss: 0.0679, Training Accuracy: 0.9766\n",
      "Iteration: 6731, Training Loss: 0.0377, Training Accuracy: 0.9922\n",
      "Iteration: 6741, Training Loss: 0.2278, Training Accuracy: 0.9453\n",
      "Iteration: 6751, Training Loss: 0.0606, Training Accuracy: 0.9922\n",
      "Iteration: 6761, Training Loss: 0.2078, Training Accuracy: 0.9688\n",
      "Iteration: 6771, Training Loss: 0.0316, Training Accuracy: 0.9844\n",
      "Iteration: 6781, Training Loss: 0.2155, Training Accuracy: 0.9844\n",
      "Iteration: 6791, Training Loss: 0.0382, Training Accuracy: 0.9922\n",
      "Iteration: 6801, Training Loss: 0.0915, Training Accuracy: 0.9688\n",
      "Iteration: 6811, Training Loss: 0.1476, Training Accuracy: 0.9531\n",
      "Iteration: 6821, Training Loss: 0.2407, Training Accuracy: 0.9609\n",
      "Iteration: 6831, Training Loss: 0.3759, Training Accuracy: 0.9609\n",
      "Iteration: 6841, Training Loss: 0.3493, Training Accuracy: 0.9453\n",
      "Iteration: 6851, Training Loss: 0.3772, Training Accuracy: 0.9219\n",
      "Iteration: 6861, Training Loss: 0.2678, Training Accuracy: 0.9531\n",
      "Iteration: 6871, Training Loss: 0.0626, Training Accuracy: 0.9766\n",
      "Iteration: 6881, Training Loss: 0.0457, Training Accuracy: 0.9844\n",
      "Iteration: 6891, Training Loss: 0.0509, Training Accuracy: 0.9688\n",
      "Iteration: 6901, Training Loss: 0.2564, Training Accuracy: 0.9844\n",
      "Iteration: 6911, Training Loss: 0.0391, Training Accuracy: 0.9922\n",
      "Iteration: 6921, Training Loss: 0.0364, Training Accuracy: 0.9922\n",
      "Iteration: 6931, Training Loss: 0.0124, Training Accuracy: 1.0000\n",
      "Iteration: 6941, Training Loss: 0.0037, Training Accuracy: 1.0000\n",
      "Iteration: 6951, Training Loss: 0.2035, Training Accuracy: 0.9766\n",
      "Iteration: 6961, Training Loss: 0.0118, Training Accuracy: 1.0000\n",
      "Iteration: 6971, Training Loss: 0.1852, Training Accuracy: 0.9688\n",
      "Iteration: 6981, Training Loss: 0.3800, Training Accuracy: 0.9531\n",
      "Iteration: 6991, Training Loss: 0.1785, Training Accuracy: 0.9453\n",
      "Iteration: 7001, Training Loss: 0.0935, Training Accuracy: 0.9609\n",
      "Iteration: 7011, Training Loss: 0.1413, Training Accuracy: 0.9609\n",
      "Iteration: 7021, Training Loss: 0.0213, Training Accuracy: 0.9922\n",
      "Iteration: 7031, Training Loss: 0.3967, Training Accuracy: 0.9375\n",
      "Iteration: 7041, Training Loss: 0.2743, Training Accuracy: 0.9688\n",
      "Iteration: 7051, Training Loss: 0.1929, Training Accuracy: 0.9688\n",
      "Iteration: 7061, Training Loss: 0.2857, Training Accuracy: 0.9062\n",
      "Iteration: 7071, Training Loss: 0.1384, Training Accuracy: 0.9609\n",
      "Iteration: 7081, Training Loss: 0.1827, Training Accuracy: 0.9453\n",
      "Iteration: 7091, Training Loss: 0.2466, Training Accuracy: 0.9297\n",
      "Iteration: 7101, Training Loss: 0.1791, Training Accuracy: 0.9453\n",
      "Iteration: 7111, Training Loss: 0.0794, Training Accuracy: 0.9766\n",
      "Iteration: 7121, Training Loss: 0.2529, Training Accuracy: 0.9609\n",
      "Iteration: 7131, Training Loss: 0.0225, Training Accuracy: 0.9922\n",
      "Iteration: 7141, Training Loss: 0.0681, Training Accuracy: 0.9844\n",
      "Iteration: 7151, Training Loss: 0.0234, Training Accuracy: 0.9922\n",
      "Iteration: 7161, Training Loss: 0.1234, Training Accuracy: 0.9609\n",
      "Iteration: 7171, Training Loss: 0.1400, Training Accuracy: 0.9531\n",
      "Iteration: 7181, Training Loss: 0.0963, Training Accuracy: 0.9766\n",
      "Iteration: 7191, Training Loss: 0.1427, Training Accuracy: 0.9609\n",
      "Iteration: 7201, Training Loss: 0.2245, Training Accuracy: 0.9688\n",
      "Iteration: 7211, Training Loss: 0.1447, Training Accuracy: 0.9531\n",
      "Iteration: 7221, Training Loss: 0.1983, Training Accuracy: 0.9688\n",
      "Iteration: 7231, Training Loss: 0.2950, Training Accuracy: 0.9609\n",
      "Iteration: 7241, Training Loss: 0.1720, Training Accuracy: 0.9453\n",
      "Iteration: 7251, Training Loss: 0.0329, Training Accuracy: 0.9922\n",
      "Iteration: 7261, Training Loss: 0.0969, Training Accuracy: 0.9766\n",
      "Iteration: 7271, Training Loss: 0.1885, Training Accuracy: 0.9922\n",
      "Iteration: 7281, Training Loss: 0.2097, Training Accuracy: 0.9609\n",
      "Iteration: 7291, Training Loss: 0.2246, Training Accuracy: 0.9766\n",
      "Iteration: 7301, Training Loss: 0.2933, Training Accuracy: 0.9688\n",
      "Iteration: 7311, Training Loss: 0.2413, Training Accuracy: 0.9766\n",
      "Iteration: 7321, Training Loss: 0.2948, Training Accuracy: 0.9609\n",
      "Iteration: 7331, Training Loss: 0.1270, Training Accuracy: 0.9766\n",
      "Iteration: 7341, Training Loss: 0.2764, Training Accuracy: 0.9766\n",
      "Iteration: 7351, Training Loss: 0.0736, Training Accuracy: 0.9844\n",
      "Iteration: 7361, Training Loss: 0.0922, Training Accuracy: 0.9766\n",
      "Iteration: 7371, Training Loss: 0.1930, Training Accuracy: 0.9531\n",
      "Iteration: 7381, Training Loss: 0.3994, Training Accuracy: 0.9219\n",
      "Iteration: 7391, Training Loss: 0.1847, Training Accuracy: 0.9531\n",
      "Iteration: 7401, Training Loss: 0.1613, Training Accuracy: 0.9531\n",
      "Iteration: 7411, Training Loss: 0.3324, Training Accuracy: 0.9531\n",
      "Iteration: 7421, Training Loss: 0.0343, Training Accuracy: 0.9844\n",
      "Iteration: 7431, Training Loss: 0.6412, Training Accuracy: 0.9297\n",
      "Iteration: 7441, Training Loss: 0.3616, Training Accuracy: 0.9609\n",
      "Iteration: 7451, Training Loss: 0.6618, Training Accuracy: 0.8828\n",
      "Iteration: 7461, Training Loss: 0.4967, Training Accuracy: 0.9453\n",
      "Iteration: 7471, Training Loss: 0.1222, Training Accuracy: 0.9688\n",
      "Iteration: 7481, Training Loss: 0.1637, Training Accuracy: 0.9453\n",
      "Iteration: 7491, Training Loss: 0.3167, Training Accuracy: 0.9453\n",
      "Iteration: 7501, Training Loss: 0.2538, Training Accuracy: 0.9766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7511, Training Loss: 0.0830, Training Accuracy: 0.9766\n",
      "Iteration: 7521, Training Loss: 0.4614, Training Accuracy: 0.9453\n",
      "Iteration: 7531, Training Loss: 0.1923, Training Accuracy: 0.9531\n",
      "Iteration: 7541, Training Loss: 0.1949, Training Accuracy: 0.9531\n",
      "Iteration: 7551, Training Loss: 0.1157, Training Accuracy: 0.9453\n",
      "Iteration: 7561, Training Loss: 0.2863, Training Accuracy: 0.9531\n",
      "Iteration: 7571, Training Loss: 0.2663, Training Accuracy: 0.9531\n",
      "Iteration: 7581, Training Loss: 0.2806, Training Accuracy: 0.9688\n",
      "Iteration: 7591, Training Loss: 0.0619, Training Accuracy: 0.9922\n",
      "Iteration: 7601, Training Loss: 0.1738, Training Accuracy: 0.9609\n",
      "Iteration: 7611, Training Loss: 0.1469, Training Accuracy: 0.9609\n",
      "Iteration: 7621, Training Loss: 0.1468, Training Accuracy: 0.9688\n",
      "Iteration: 7631, Training Loss: 0.1874, Training Accuracy: 0.9531\n",
      "Iteration: 7641, Training Loss: 0.0846, Training Accuracy: 0.9766\n",
      "Iteration: 7651, Training Loss: 0.4776, Training Accuracy: 0.9531\n",
      "Iteration: 7661, Training Loss: 0.2753, Training Accuracy: 0.9531\n",
      "Iteration: 7671, Training Loss: 0.0826, Training Accuracy: 0.9844\n",
      "Iteration: 7681, Training Loss: 0.1469, Training Accuracy: 0.9688\n",
      "Iteration: 7691, Training Loss: 0.3018, Training Accuracy: 0.9375\n",
      "Iteration: 7701, Training Loss: 0.1934, Training Accuracy: 0.9688\n",
      "Iteration: 7711, Training Loss: 0.1407, Training Accuracy: 0.9609\n",
      "Iteration: 7721, Training Loss: 0.0732, Training Accuracy: 0.9766\n",
      "Iteration: 7731, Training Loss: 0.0695, Training Accuracy: 0.9844\n",
      "Iteration: 7741, Training Loss: 0.1547, Training Accuracy: 0.9609\n",
      "Iteration: 7751, Training Loss: 0.2409, Training Accuracy: 0.9453\n",
      "Iteration: 7761, Training Loss: 0.3469, Training Accuracy: 0.9453\n",
      "Iteration: 7771, Training Loss: 0.2363, Training Accuracy: 0.9531\n",
      "Iteration: 7781, Training Loss: 0.3048, Training Accuracy: 0.9141\n",
      "Iteration: 7791, Training Loss: 0.1900, Training Accuracy: 0.9453\n",
      "Iteration: 7801, Training Loss: 0.2010, Training Accuracy: 0.9453\n",
      "Iteration: 7811, Training Loss: 0.2960, Training Accuracy: 0.9688\n",
      "Iteration: 7821, Training Loss: 0.1775, Training Accuracy: 0.9531\n",
      "Iteration: 7831, Training Loss: 0.0469, Training Accuracy: 0.9922\n",
      "Iteration: 7841, Training Loss: 0.3944, Training Accuracy: 0.9766\n",
      "Iteration: 7851, Training Loss: 0.1188, Training Accuracy: 0.9844\n",
      "Iteration: 7861, Training Loss: 0.0597, Training Accuracy: 0.9844\n",
      "Iteration: 7871, Training Loss: 0.3881, Training Accuracy: 0.9375\n",
      "Iteration: 7881, Training Loss: 0.3060, Training Accuracy: 0.9219\n",
      "Iteration: 7891, Training Loss: 0.8265, Training Accuracy: 0.9141\n",
      "Iteration: 7901, Training Loss: 0.1553, Training Accuracy: 0.9531\n",
      "Iteration: 7911, Training Loss: 0.6057, Training Accuracy: 0.9453\n",
      "Iteration: 7921, Training Loss: 0.1711, Training Accuracy: 0.9766\n",
      "Iteration: 7931, Training Loss: 0.2147, Training Accuracy: 0.9531\n",
      "Iteration: 7941, Training Loss: 0.4231, Training Accuracy: 0.9375\n",
      "Iteration: 7951, Training Loss: 0.2521, Training Accuracy: 0.9219\n",
      "Iteration: 7961, Training Loss: 0.1668, Training Accuracy: 0.9531\n",
      "Iteration: 7971, Training Loss: 0.5910, Training Accuracy: 0.8906\n",
      "Iteration: 7981, Training Loss: 0.2491, Training Accuracy: 0.9688\n",
      "Iteration: 7991, Training Loss: 0.1010, Training Accuracy: 0.9609\n",
      "Iteration: 8001, Training Loss: 0.2261, Training Accuracy: 0.9609\n",
      "Iteration: 8011, Training Loss: 0.0970, Training Accuracy: 0.9688\n",
      "Iteration: 8021, Training Loss: 0.3301, Training Accuracy: 0.9297\n",
      "Iteration: 8031, Training Loss: 0.3084, Training Accuracy: 0.9219\n",
      "Iteration: 8041, Training Loss: 0.2297, Training Accuracy: 0.9453\n",
      "Iteration: 8051, Training Loss: 0.2341, Training Accuracy: 0.9375\n",
      "Iteration: 8061, Training Loss: 0.0487, Training Accuracy: 0.9922\n",
      "Iteration: 8071, Training Loss: 0.1634, Training Accuracy: 0.9531\n",
      "Iteration: 8081, Training Loss: 0.1372, Training Accuracy: 0.9844\n",
      "Iteration: 8091, Training Loss: 0.4490, Training Accuracy: 0.9375\n",
      "Iteration: 8101, Training Loss: 0.2651, Training Accuracy: 0.9609\n",
      "Iteration: 8111, Training Loss: 0.6245, Training Accuracy: 0.9297\n",
      "Iteration: 8121, Training Loss: 0.1352, Training Accuracy: 0.9688\n",
      "Iteration: 8131, Training Loss: 0.5479, Training Accuracy: 0.9219\n",
      "Iteration: 8141, Training Loss: 0.2646, Training Accuracy: 0.9688\n",
      "Iteration: 8151, Training Loss: 0.1993, Training Accuracy: 0.9688\n",
      "Iteration: 8161, Training Loss: 0.3547, Training Accuracy: 0.9297\n",
      "Iteration: 8171, Training Loss: 0.1608, Training Accuracy: 0.9531\n",
      "Iteration: 8181, Training Loss: 0.1157, Training Accuracy: 0.9609\n",
      "Iteration: 8191, Training Loss: 0.0990, Training Accuracy: 0.9531\n",
      "Iteration: 8201, Training Loss: 0.1084, Training Accuracy: 0.9766\n",
      "Iteration: 8211, Training Loss: 0.1321, Training Accuracy: 0.9609\n",
      "Iteration: 8221, Training Loss: 0.1458, Training Accuracy: 0.9766\n",
      "Iteration: 8231, Training Loss: 0.4814, Training Accuracy: 0.9531\n",
      "Iteration: 8241, Training Loss: 0.0926, Training Accuracy: 0.9766\n",
      "Iteration: 8251, Training Loss: 0.1783, Training Accuracy: 0.9609\n",
      "Iteration: 8261, Training Loss: 0.0187, Training Accuracy: 0.9922\n",
      "Iteration: 8271, Training Loss: 0.2638, Training Accuracy: 0.9531\n",
      "Iteration: 8281, Training Loss: 0.1025, Training Accuracy: 0.9766\n",
      "Iteration: 8291, Training Loss: 0.2232, Training Accuracy: 0.9375\n",
      "Iteration: 8301, Training Loss: 0.0816, Training Accuracy: 0.9766\n",
      "Iteration: 8311, Training Loss: 0.1086, Training Accuracy: 0.9766\n",
      "Iteration: 8321, Training Loss: 0.2581, Training Accuracy: 0.9375\n",
      "Iteration: 8331, Training Loss: 0.0150, Training Accuracy: 0.9922\n",
      "Iteration: 8341, Training Loss: 0.2955, Training Accuracy: 0.9766\n",
      "Iteration: 8351, Training Loss: 0.3023, Training Accuracy: 0.9766\n",
      "Iteration: 8361, Training Loss: 0.2853, Training Accuracy: 0.9531\n",
      "Iteration: 8371, Training Loss: 0.1644, Training Accuracy: 0.9609\n",
      "Iteration: 8381, Training Loss: 0.5713, Training Accuracy: 0.9531\n",
      "Iteration: 8391, Training Loss: 0.2209, Training Accuracy: 0.9375\n",
      "Iteration: 8401, Training Loss: 0.0511, Training Accuracy: 0.9766\n",
      "Iteration: 8411, Training Loss: 0.1013, Training Accuracy: 0.9688\n",
      "Iteration: 8421, Training Loss: 0.3535, Training Accuracy: 0.9688\n",
      "Iteration: 8431, Training Loss: 0.2810, Training Accuracy: 0.9297\n",
      "Iteration: 8441, Training Loss: 0.1998, Training Accuracy: 0.9766\n",
      "Iteration: 8451, Training Loss: 0.5975, Training Accuracy: 0.9297\n",
      "Iteration: 8461, Training Loss: 0.2243, Training Accuracy: 0.9609\n",
      "Iteration: 8471, Training Loss: 0.2964, Training Accuracy: 0.9531\n",
      "Iteration: 8481, Training Loss: 0.1067, Training Accuracy: 0.9609\n",
      "Iteration: 8491, Training Loss: 0.0526, Training Accuracy: 0.9844\n",
      "Iteration: 8501, Training Loss: 0.1206, Training Accuracy: 0.9609\n",
      "Iteration: 8511, Training Loss: 0.0717, Training Accuracy: 0.9844\n",
      "Iteration: 8521, Training Loss: 0.3756, Training Accuracy: 0.9688\n",
      "Iteration: 8531, Training Loss: 0.3102, Training Accuracy: 0.9531\n",
      "Iteration: 8541, Training Loss: 0.1099, Training Accuracy: 0.9688\n",
      "Iteration: 8551, Training Loss: 0.2595, Training Accuracy: 0.9219\n",
      "Iteration: 8561, Training Loss: 0.0523, Training Accuracy: 0.9844\n",
      "Iteration: 8571, Training Loss: 0.2948, Training Accuracy: 0.9531\n",
      "Iteration: 8581, Training Loss: 0.4247, Training Accuracy: 0.9453\n",
      "Iteration: 8591, Training Loss: 0.2648, Training Accuracy: 0.9453\n",
      "Iteration: 8601, Training Loss: 0.2535, Training Accuracy: 0.9297\n",
      "Iteration: 8611, Training Loss: 0.1314, Training Accuracy: 0.9766\n",
      "Iteration: 8621, Training Loss: 0.3446, Training Accuracy: 0.9375\n",
      "Iteration: 8631, Training Loss: 0.3043, Training Accuracy: 0.8828\n",
      "Iteration: 8641, Training Loss: 0.7797, Training Accuracy: 0.8984\n",
      "Iteration: 8651, Training Loss: 0.2798, Training Accuracy: 0.8984\n",
      "Iteration: 8661, Training Loss: 0.2426, Training Accuracy: 0.9219\n",
      "Iteration: 8671, Training Loss: 0.3094, Training Accuracy: 0.9531\n",
      "Iteration: 8681, Training Loss: 0.4458, Training Accuracy: 0.8906\n",
      "Iteration: 8691, Training Loss: 0.4557, Training Accuracy: 0.9062\n",
      "Iteration: 8701, Training Loss: 0.1862, Training Accuracy: 0.9688\n",
      "Iteration: 8711, Training Loss: 0.3292, Training Accuracy: 0.9453\n",
      "Iteration: 8721, Training Loss: 0.2155, Training Accuracy: 0.9453\n",
      "Iteration: 8731, Training Loss: 0.3517, Training Accuracy: 0.9375\n",
      "Iteration: 8741, Training Loss: 0.2412, Training Accuracy: 0.9297\n",
      "Iteration: 8751, Training Loss: 0.3490, Training Accuracy: 0.9531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8761, Training Loss: 0.5426, Training Accuracy: 0.9297\n",
      "Iteration: 8771, Training Loss: 0.3668, Training Accuracy: 0.9453\n",
      "Iteration: 8781, Training Loss: 0.3064, Training Accuracy: 0.9453\n",
      "Iteration: 8791, Training Loss: 0.3874, Training Accuracy: 0.9375\n",
      "Iteration: 8801, Training Loss: 0.1824, Training Accuracy: 0.9609\n",
      "Iteration: 8811, Training Loss: 0.2293, Training Accuracy: 0.9531\n",
      "Iteration: 8821, Training Loss: 0.1231, Training Accuracy: 0.9688\n",
      "Iteration: 8831, Training Loss: 0.1902, Training Accuracy: 0.9609\n",
      "Iteration: 8841, Training Loss: 0.1688, Training Accuracy: 0.9688\n",
      "Iteration: 8851, Training Loss: 0.5014, Training Accuracy: 0.9453\n",
      "Iteration: 8861, Training Loss: 0.3398, Training Accuracy: 0.9688\n",
      "Iteration: 8871, Training Loss: 0.3790, Training Accuracy: 0.9688\n",
      "Iteration: 8881, Training Loss: 0.6026, Training Accuracy: 0.9688\n",
      "Iteration: 8891, Training Loss: 0.3322, Training Accuracy: 0.9531\n",
      "Iteration: 8901, Training Loss: 0.2488, Training Accuracy: 0.9297\n",
      "Iteration: 8911, Training Loss: 0.3211, Training Accuracy: 0.9375\n",
      "Iteration: 8921, Training Loss: 0.2287, Training Accuracy: 0.9453\n",
      "Iteration: 8931, Training Loss: 0.1505, Training Accuracy: 0.9609\n",
      "Iteration: 8941, Training Loss: 0.4933, Training Accuracy: 0.9297\n",
      "Iteration: 8951, Training Loss: 0.2214, Training Accuracy: 0.9688\n",
      "Iteration: 8961, Training Loss: 0.2115, Training Accuracy: 0.9453\n",
      "Iteration: 8971, Training Loss: 0.4563, Training Accuracy: 0.9297\n",
      "Iteration: 8981, Training Loss: 1.6305, Training Accuracy: 0.9453\n",
      "Iteration: 8991, Training Loss: 0.1988, Training Accuracy: 0.9609\n",
      "Iteration: 9001, Training Loss: 0.2285, Training Accuracy: 0.9297\n",
      "Iteration: 9011, Training Loss: 0.3891, Training Accuracy: 0.8984\n",
      "Iteration: 9021, Training Loss: 0.2218, Training Accuracy: 0.9375\n",
      "Iteration: 9031, Training Loss: 0.4988, Training Accuracy: 0.8516\n",
      "Iteration: 9041, Training Loss: 0.3001, Training Accuracy: 0.9141\n",
      "Iteration: 9051, Training Loss: 0.4626, Training Accuracy: 0.9141\n",
      "Iteration: 9061, Training Loss: 0.4070, Training Accuracy: 0.9141\n",
      "Iteration: 9071, Training Loss: 0.2813, Training Accuracy: 0.8906\n",
      "Iteration: 9081, Training Loss: 0.5041, Training Accuracy: 0.8750\n",
      "Iteration: 9091, Training Loss: 0.2743, Training Accuracy: 0.9375\n",
      "Iteration: 9101, Training Loss: 0.2928, Training Accuracy: 0.9688\n",
      "Iteration: 9111, Training Loss: 0.5087, Training Accuracy: 0.9375\n",
      "Iteration: 9121, Training Loss: 0.3693, Training Accuracy: 0.9062\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ea6feb21c905>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mlosscalc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracycalc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmerged_summary_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iteration: %d, Training Loss: %0.4f, Training Accuracy: %0.4f\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosscalc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracycalc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step=1\n",
    "    while step <= num_of_iters:       \n",
    "        batch_x, batch_y = mnist.train.next_batch(batchsize)  \n",
    "        sess.run(train_min, feed_dict={X:batch_x, Y:batch_y})        \n",
    "        losscalc, accuracycalc, merged_summary = sess.run([loss, accuracy,merged_summary_op], feed_dict={X:batch_x, Y:batch_y})\n",
    "        if (step%10==1):\n",
    "            print(\"Iteration: %d, Training Loss: %0.4f, Training Accuracy: %0.4f\"%(step, losscalc, accuracycalc))\n",
    "        writer.add_summary(merged_summary, step)\n",
    "        step += 1\n",
    "        \n",
    "    test_x, test_y = mnist.test.next_batch(10000)\n",
    "    losscalc, accuracycalc = sess.run([loss, accuracy], feed_dict={X:test_x, Y:test_y})\n",
    "    print(\"Iterations: %d, Test Loss: %0.4f, Test Accuracy: %0.4f\"%(step, losscalc, accuracycalc))\n",
    "\n",
    "writer.close()               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
