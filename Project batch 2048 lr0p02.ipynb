{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tf-cpu/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "55000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "print(mnist.train.num_examples) # Number of training data\n",
    "print(mnist.test.num_examples) # Number of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "learningrate = 0.02\n",
    "num_of_epochs=50\n",
    "batchsize = 2048\n",
    "num_of_iters=55000/batchsize*num_of_epochs\n",
    "\n",
    "\n",
    "noutput = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, noutput])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # reshape input to 28x28 size\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution layer 1\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    # Max pooling\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution layer 2\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    # Max pooling\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "\n",
    "init_var=tf.contrib.layers.variance_scaling_initializer()\n",
    "init_xavier=tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "weights = {\n",
    "    'wc1': tf.get_variable(\"wc1\", shape=[5, 5, 1, 32],initializer=init_var),\n",
    "    'wc2': tf.get_variable(\"wc2\", shape=[5, 5, 32, 64],initializer=init_var),\n",
    "    'wd1': tf.get_variable(\"wd1\", shape=[7*7*64, 1024],initializer=init_var),\n",
    "    'out': tf.get_variable(\"out\", shape=[1024,noutput],initializer=init_var)\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.zeros([32])),\n",
    "    'bc2': tf.Variable(tf.zeros([64])),\n",
    "    'bd1': tf.Variable(tf.zeros([1024])),\n",
    "    'out': tf.Variable(tf.zeros([noutput]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = conv_net(X, weights, biases, dropout=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = model, labels=Y))\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=learningrate)\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "optimizer=tf.train.MomentumOptimizer(learning_rate=learningrate, momentum=0.99)\n",
    "\n",
    "train_min = optimizer.minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "tf.summary.scalar(\"trainingLoss\", loss)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "filename = \"./log/mnist_project_batchsize2048\" \n",
    "writer = tf.summary.FileWriter(filename, tf.get_default_graph())\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1, Training Loss: 2.4870, Training Accuracy: 0.1108\n",
      "Iteration: 11, Training Loss: 0.7405, Training Accuracy: 0.7598\n",
      "Iteration: 21, Training Loss: 0.4817, Training Accuracy: 0.8667\n",
      "Iteration: 31, Training Loss: 0.3306, Training Accuracy: 0.9028\n",
      "Iteration: 41, Training Loss: 0.2252, Training Accuracy: 0.9365\n",
      "Iteration: 51, Training Loss: 0.1851, Training Accuracy: 0.9482\n",
      "Iteration: 61, Training Loss: 0.1482, Training Accuracy: 0.9521\n",
      "Iteration: 71, Training Loss: 0.1906, Training Accuracy: 0.9531\n",
      "Iteration: 81, Training Loss: 0.1351, Training Accuracy: 0.9648\n",
      "Iteration: 91, Training Loss: 0.1111, Training Accuracy: 0.9678\n",
      "Iteration: 101, Training Loss: 0.1104, Training Accuracy: 0.9648\n",
      "Iteration: 111, Training Loss: 0.0913, Training Accuracy: 0.9756\n",
      "Iteration: 121, Training Loss: 0.0838, Training Accuracy: 0.9790\n",
      "Iteration: 131, Training Loss: 0.0826, Training Accuracy: 0.9766\n",
      "Iteration: 141, Training Loss: 0.0480, Training Accuracy: 0.9834\n",
      "Iteration: 151, Training Loss: 0.0623, Training Accuracy: 0.9839\n",
      "Iteration: 161, Training Loss: 0.0737, Training Accuracy: 0.9751\n",
      "Iteration: 171, Training Loss: 0.0837, Training Accuracy: 0.9800\n",
      "Iteration: 181, Training Loss: 0.0449, Training Accuracy: 0.9873\n",
      "Iteration: 191, Training Loss: 0.0666, Training Accuracy: 0.9849\n",
      "Iteration: 201, Training Loss: 0.0624, Training Accuracy: 0.9829\n",
      "Iteration: 211, Training Loss: 0.0574, Training Accuracy: 0.9819\n",
      "Iteration: 221, Training Loss: 0.0268, Training Accuracy: 0.9868\n",
      "Iteration: 231, Training Loss: 0.0334, Training Accuracy: 0.9888\n",
      "Iteration: 241, Training Loss: 0.0360, Training Accuracy: 0.9893\n",
      "Iteration: 251, Training Loss: 0.0370, Training Accuracy: 0.9858\n",
      "Iteration: 261, Training Loss: 0.0273, Training Accuracy: 0.9912\n",
      "Iteration: 271, Training Loss: 0.0356, Training Accuracy: 0.9893\n",
      "Iteration: 281, Training Loss: 0.0367, Training Accuracy: 0.9902\n",
      "Iteration: 291, Training Loss: 0.0299, Training Accuracy: 0.9893\n",
      "Iteration: 301, Training Loss: 0.0268, Training Accuracy: 0.9917\n",
      "Iteration: 311, Training Loss: 0.0161, Training Accuracy: 0.9941\n",
      "Iteration: 321, Training Loss: 0.0182, Training Accuracy: 0.9941\n",
      "Iteration: 331, Training Loss: 0.0114, Training Accuracy: 0.9966\n",
      "Iteration: 341, Training Loss: 0.0173, Training Accuracy: 0.9932\n",
      "Iteration: 351, Training Loss: 0.0133, Training Accuracy: 0.9971\n",
      "Iteration: 361, Training Loss: 0.0219, Training Accuracy: 0.9941\n",
      "Iteration: 371, Training Loss: 0.0239, Training Accuracy: 0.9922\n",
      "Iteration: 381, Training Loss: 0.0176, Training Accuracy: 0.9956\n",
      "Iteration: 391, Training Loss: 0.0180, Training Accuracy: 0.9941\n",
      "Iteration: 401, Training Loss: 0.0091, Training Accuracy: 0.9971\n",
      "Iteration: 411, Training Loss: 0.0215, Training Accuracy: 0.9922\n",
      "Iteration: 421, Training Loss: 0.0124, Training Accuracy: 0.9946\n",
      "Iteration: 431, Training Loss: 0.0144, Training Accuracy: 0.9956\n",
      "Iteration: 441, Training Loss: 0.0070, Training Accuracy: 0.9971\n",
      "Iteration: 451, Training Loss: 0.0101, Training Accuracy: 0.9976\n",
      "Iteration: 461, Training Loss: 0.0107, Training Accuracy: 0.9956\n",
      "Iteration: 471, Training Loss: 0.0083, Training Accuracy: 0.9971\n",
      "Iteration: 481, Training Loss: 0.0070, Training Accuracy: 0.9976\n",
      "Iteration: 491, Training Loss: 0.0117, Training Accuracy: 0.9956\n",
      "Iteration: 501, Training Loss: 0.0056, Training Accuracy: 0.9980\n",
      "Iteration: 511, Training Loss: 0.0029, Training Accuracy: 0.9995\n",
      "Iteration: 521, Training Loss: 0.0063, Training Accuracy: 0.9980\n",
      "Iteration: 531, Training Loss: 0.0068, Training Accuracy: 0.9966\n",
      "Iteration: 541, Training Loss: 0.0095, Training Accuracy: 0.9951\n",
      "Iteration: 551, Training Loss: 0.0066, Training Accuracy: 0.9980\n",
      "Iteration: 561, Training Loss: 0.0033, Training Accuracy: 0.9990\n",
      "Iteration: 571, Training Loss: 0.0115, Training Accuracy: 0.9961\n",
      "Iteration: 581, Training Loss: 0.0063, Training Accuracy: 0.9980\n",
      "Iteration: 591, Training Loss: 0.0054, Training Accuracy: 0.9990\n",
      "Iteration: 601, Training Loss: 0.0038, Training Accuracy: 0.9990\n",
      "Iteration: 611, Training Loss: 0.0093, Training Accuracy: 0.9976\n",
      "Iteration: 621, Training Loss: 0.0066, Training Accuracy: 0.9985\n",
      "Iteration: 631, Training Loss: 0.0087, Training Accuracy: 0.9976\n",
      "Iteration: 641, Training Loss: 0.0066, Training Accuracy: 0.9980\n",
      "Iteration: 651, Training Loss: 0.0054, Training Accuracy: 0.9985\n",
      "Iteration: 661, Training Loss: 0.0038, Training Accuracy: 0.9985\n",
      "Iteration: 671, Training Loss: 0.0068, Training Accuracy: 0.9971\n",
      "Iteration: 681, Training Loss: 0.0048, Training Accuracy: 0.9985\n",
      "Iteration: 691, Training Loss: 0.0027, Training Accuracy: 0.9995\n",
      "Iteration: 701, Training Loss: 0.0037, Training Accuracy: 0.9985\n",
      "Iteration: 711, Training Loss: 0.0067, Training Accuracy: 0.9980\n",
      "Iteration: 721, Training Loss: 0.0035, Training Accuracy: 0.9990\n",
      "Iteration: 731, Training Loss: 0.0049, Training Accuracy: 0.9980\n",
      "Iteration: 741, Training Loss: 0.0025, Training Accuracy: 1.0000\n",
      "Iteration: 751, Training Loss: 0.0056, Training Accuracy: 0.9976\n",
      "Iteration: 761, Training Loss: 0.0015, Training Accuracy: 1.0000\n",
      "Iteration: 771, Training Loss: 0.0048, Training Accuracy: 0.9971\n",
      "Iteration: 781, Training Loss: 0.0056, Training Accuracy: 0.9985\n",
      "Iteration: 791, Training Loss: 0.0028, Training Accuracy: 0.9985\n",
      "Iteration: 801, Training Loss: 0.0069, Training Accuracy: 0.9985\n",
      "Iteration: 811, Training Loss: 0.0015, Training Accuracy: 1.0000\n",
      "Iteration: 821, Training Loss: 0.0054, Training Accuracy: 0.9990\n",
      "Iteration: 831, Training Loss: 0.0028, Training Accuracy: 0.9995\n",
      "Iteration: 841, Training Loss: 0.0058, Training Accuracy: 0.9990\n",
      "Iteration: 851, Training Loss: 0.0012, Training Accuracy: 1.0000\n",
      "Iteration: 861, Training Loss: 0.0021, Training Accuracy: 1.0000\n",
      "Iteration: 871, Training Loss: 0.0046, Training Accuracy: 0.9980\n",
      "Iteration: 881, Training Loss: 0.0014, Training Accuracy: 1.0000\n",
      "Iteration: 891, Training Loss: 0.0027, Training Accuracy: 0.9990\n",
      "Iteration: 901, Training Loss: 0.0048, Training Accuracy: 0.9976\n",
      "Iteration: 911, Training Loss: 0.0019, Training Accuracy: 0.9995\n",
      "Iteration: 921, Training Loss: 0.0015, Training Accuracy: 1.0000\n",
      "Iteration: 931, Training Loss: 0.0030, Training Accuracy: 0.9990\n",
      "Iteration: 941, Training Loss: 0.0030, Training Accuracy: 0.9985\n",
      "Iteration: 951, Training Loss: 0.0029, Training Accuracy: 0.9990\n",
      "Iteration: 961, Training Loss: 0.0029, Training Accuracy: 0.9990\n",
      "Iteration: 971, Training Loss: 0.0031, Training Accuracy: 0.9990\n",
      "Iteration: 981, Training Loss: 0.0033, Training Accuracy: 0.9990\n",
      "Iteration: 991, Training Loss: 0.0010, Training Accuracy: 0.9995\n",
      "Iteration: 1001, Training Loss: 0.0013, Training Accuracy: 1.0000\n",
      "Iteration: 1011, Training Loss: 0.0022, Training Accuracy: 0.9995\n",
      "Iteration: 1021, Training Loss: 0.0011, Training Accuracy: 1.0000\n",
      "Iteration: 1031, Training Loss: 0.0035, Training Accuracy: 0.9990\n",
      "Iteration: 1041, Training Loss: 0.0036, Training Accuracy: 0.9985\n",
      "Iteration: 1051, Training Loss: 0.0017, Training Accuracy: 0.9995\n",
      "Iteration: 1061, Training Loss: 0.0018, Training Accuracy: 0.9995\n",
      "Iteration: 1071, Training Loss: 0.0021, Training Accuracy: 0.9995\n",
      "Iteration: 1081, Training Loss: 0.0012, Training Accuracy: 1.0000\n",
      "Iteration: 1091, Training Loss: 0.0032, Training Accuracy: 0.9985\n",
      "Iteration: 1101, Training Loss: 0.0025, Training Accuracy: 0.9985\n",
      "Iteration: 1111, Training Loss: 0.0033, Training Accuracy: 0.9985\n",
      "Iteration: 1121, Training Loss: 0.0004, Training Accuracy: 1.0000\n",
      "Iteration: 1131, Training Loss: 0.0009, Training Accuracy: 1.0000\n",
      "Iteration: 1141, Training Loss: 0.0010, Training Accuracy: 1.0000\n",
      "Iteration: 1151, Training Loss: 0.0014, Training Accuracy: 1.0000\n",
      "Iteration: 1161, Training Loss: 0.0033, Training Accuracy: 0.9995\n",
      "Iteration: 1171, Training Loss: 0.0009, Training Accuracy: 1.0000\n",
      "Iteration: 1181, Training Loss: 0.0016, Training Accuracy: 0.9995\n",
      "Iteration: 1191, Training Loss: 0.0020, Training Accuracy: 0.9995\n",
      "Iteration: 1201, Training Loss: 0.0007, Training Accuracy: 1.0000\n",
      "Iteration: 1211, Training Loss: 0.0004, Training Accuracy: 1.0000\n",
      "Iteration: 1221, Training Loss: 0.0024, Training Accuracy: 0.9990\n",
      "Iteration: 1231, Training Loss: 0.0012, Training Accuracy: 0.9995\n",
      "Iteration: 1241, Training Loss: 0.0023, Training Accuracy: 0.9995\n",
      "Iteration: 1251, Training Loss: 0.0018, Training Accuracy: 0.9995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1261, Training Loss: 0.0019, Training Accuracy: 0.9995\n",
      "Iteration: 1271, Training Loss: 0.0004, Training Accuracy: 1.0000\n",
      "Iteration: 1281, Training Loss: 0.0019, Training Accuracy: 0.9990\n",
      "Iteration: 1291, Training Loss: 0.0009, Training Accuracy: 0.9995\n",
      "Iteration: 1301, Training Loss: 0.0009, Training Accuracy: 1.0000\n",
      "Iteration: 1311, Training Loss: 0.0010, Training Accuracy: 1.0000\n",
      "Iteration: 1321, Training Loss: 0.0004, Training Accuracy: 1.0000\n",
      "Iteration: 1331, Training Loss: 0.0011, Training Accuracy: 0.9995\n",
      "Iteration: 1341, Training Loss: 0.0010, Training Accuracy: 1.0000\n",
      "Iterations: 1343, Test Loss: 0.0363, Test Accuracy: 0.9923\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step=1\n",
    "    while step <= num_of_iters:       \n",
    "        batch_x, batch_y = mnist.train.next_batch(batchsize)  \n",
    "        sess.run(train_min, feed_dict={X:batch_x, Y:batch_y})        \n",
    "        losscalc, accuracycalc, merged_summary = sess.run([loss, accuracy,merged_summary_op], feed_dict={X:batch_x, Y:batch_y})\n",
    "        if (step%10==1):\n",
    "            print(\"Iteration: %d, Training Loss: %0.4f, Training Accuracy: %0.4f\"%(step, losscalc, accuracycalc))\n",
    "        writer.add_summary(merged_summary, step)\n",
    "        step += 1\n",
    "        \n",
    "    test_x, test_y = mnist.test.next_batch(10000)\n",
    "    losscalc, accuracycalc = sess.run([loss, accuracy], feed_dict={X:test_x, Y:test_y})\n",
    "    print(\"Iterations: %d, Test Loss: %0.4f, Test Accuracy: %0.4f\"%(step, losscalc, accuracycalc))\n",
    "\n",
    "writer.close()               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
