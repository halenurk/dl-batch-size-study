{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tf-cpu/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "55000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "print(mnist.train.num_examples) # Number of training data\n",
    "print(mnist.test.num_examples) # Number of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "learningrate = 0.02\n",
    "num_of_epochs=50\n",
    "batchsize = 8192\n",
    "num_of_iters=55000/batchsize*num_of_epochs\n",
    "\n",
    "\n",
    "noutput = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, noutput])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # reshape input to 28x28 size\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution layer 1\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    # Max pooling\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution layer 2\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    # Max pooling\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "\n",
    "init_var=tf.contrib.layers.variance_scaling_initializer()\n",
    "init_xavier=tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "weights = {\n",
    "    'wc1': tf.get_variable(\"wc1\", shape=[5, 5, 1, 32],initializer=init_var),\n",
    "    'wc2': tf.get_variable(\"wc2\", shape=[5, 5, 32, 64],initializer=init_var),\n",
    "    'wd1': tf.get_variable(\"wd1\", shape=[7*7*64, 1024],initializer=init_var),\n",
    "    'out': tf.get_variable(\"out\", shape=[1024,noutput],initializer=init_var)\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.zeros([32])),\n",
    "    'bc2': tf.Variable(tf.zeros([64])),\n",
    "    'bd1': tf.Variable(tf.zeros([1024])),\n",
    "    'out': tf.Variable(tf.zeros([noutput]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = conv_net(X, weights, biases, dropout=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = model, labels=Y))\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=learningrate)\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "optimizer=tf.train.MomentumOptimizer(learning_rate=learningrate, momentum=0.99)\n",
    "\n",
    "train_min = optimizer.minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "tf.summary.scalar(\"trainingLoss\", loss)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "filename = \"./log/mnist_project_batchsize8192lr0p04\" \n",
    "writer = tf.summary.FileWriter(filename, tf.get_default_graph())\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1, Training Loss: 2.4874, Training Accuracy: 0.1318\n",
      "Iteration: 11, Training Loss: 0.6675, Training Accuracy: 0.7788\n",
      "Iteration: 21, Training Loss: 0.4369, Training Accuracy: 0.8826\n",
      "Iteration: 31, Training Loss: 0.2853, Training Accuracy: 0.9170\n",
      "Iteration: 41, Training Loss: 0.2087, Training Accuracy: 0.9360\n",
      "Iteration: 51, Training Loss: 0.1592, Training Accuracy: 0.9471\n",
      "Iteration: 61, Training Loss: 0.1376, Training Accuracy: 0.9602\n",
      "Iteration: 71, Training Loss: 0.1206, Training Accuracy: 0.9623\n",
      "Iteration: 81, Training Loss: 0.1029, Training Accuracy: 0.9669\n",
      "Iteration: 91, Training Loss: 0.0936, Training Accuracy: 0.9720\n",
      "Iteration: 101, Training Loss: 0.0818, Training Accuracy: 0.9762\n",
      "Iteration: 111, Training Loss: 0.0636, Training Accuracy: 0.9812\n",
      "Iteration: 121, Training Loss: 0.0594, Training Accuracy: 0.9816\n",
      "Iteration: 131, Training Loss: 0.0563, Training Accuracy: 0.9845\n",
      "Iteration: 141, Training Loss: 0.0604, Training Accuracy: 0.9818\n",
      "Iteration: 151, Training Loss: 0.0435, Training Accuracy: 0.9855\n",
      "Iteration: 161, Training Loss: 0.0448, Training Accuracy: 0.9871\n",
      "Iteration: 171, Training Loss: 0.0482, Training Accuracy: 0.9847\n",
      "Iteration: 181, Training Loss: 0.0406, Training Accuracy: 0.9871\n",
      "Iteration: 191, Training Loss: 0.0357, Training Accuracy: 0.9899\n",
      "Iteration: 201, Training Loss: 0.0372, Training Accuracy: 0.9878\n",
      "Iteration: 211, Training Loss: 0.0310, Training Accuracy: 0.9896\n",
      "Iteration: 221, Training Loss: 0.0326, Training Accuracy: 0.9897\n",
      "Iteration: 231, Training Loss: 0.0316, Training Accuracy: 0.9907\n",
      "Iteration: 241, Training Loss: 0.0252, Training Accuracy: 0.9926\n",
      "Iteration: 251, Training Loss: 0.0201, Training Accuracy: 0.9932\n",
      "Iteration: 261, Training Loss: 0.0199, Training Accuracy: 0.9930\n",
      "Iteration: 271, Training Loss: 0.0231, Training Accuracy: 0.9924\n",
      "Iteration: 281, Training Loss: 0.0200, Training Accuracy: 0.9934\n",
      "Iteration: 291, Training Loss: 0.0158, Training Accuracy: 0.9946\n",
      "Iteration: 301, Training Loss: 0.0151, Training Accuracy: 0.9949\n",
      "Iteration: 311, Training Loss: 0.0147, Training Accuracy: 0.9951\n",
      "Iteration: 321, Training Loss: 0.0166, Training Accuracy: 0.9951\n",
      "Iteration: 331, Training Loss: 0.0119, Training Accuracy: 0.9958\n",
      "Iterations: 336, Test Loss: 0.0299, Test Accuracy: 0.9903\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step=1\n",
    "    while step <= num_of_iters:       \n",
    "        batch_x, batch_y = mnist.train.next_batch(batchsize)  \n",
    "        sess.run(train_min, feed_dict={X:batch_x, Y:batch_y})        \n",
    "        losscalc, accuracycalc, merged_summary = sess.run([loss, accuracy,merged_summary_op], feed_dict={X:batch_x, Y:batch_y})\n",
    "        if (step%10==1):\n",
    "            print(\"Iteration: %d, Training Loss: %0.4f, Training Accuracy: %0.4f\"%(step, losscalc, accuracycalc))\n",
    "        writer.add_summary(merged_summary, step)\n",
    "        step += 1\n",
    "        \n",
    "    test_x, test_y = mnist.test.next_batch(10000)\n",
    "    losscalc, accuracycalc = sess.run([loss, accuracy], feed_dict={X:test_x, Y:test_y})\n",
    "    print(\"Iterations: %d, Test Loss: %0.4f, Test Accuracy: %0.4f\"%(step, losscalc, accuracycalc))\n",
    "\n",
    "writer.close()               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
